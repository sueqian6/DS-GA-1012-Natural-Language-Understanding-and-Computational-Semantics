{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yq729-HW-2 Sentiment Analysis with DANs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "py36"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "z_USwYM3yiDa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# HW 2: Deep Bag-of-Words\n",
        "\n",
        "![words.jpeg](https://cdn-images-1.medium.com/max/1600/0*JpqZhCNsQ_OGaRkB.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "In this homework, you will be implementing a deep averaging network, detailed in [Deep Unordered Composition  Rivals Syntactic Methods for Text Classification by Iyyer et al. (2015)](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf) and training it to do sentiment analysis on the Stanford Sentiment Treebank.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Please use all of the starter code that is provided, do not make any changes to the data processing, evaluation, and training functions. Only add code were you're asked to.**\n",
        "\n",
        "<br>"
      ]
    },
    {
      "metadata": {
        "id": "5Ze-bk8VyiDd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 0: Read Paper!\n",
        "\n",
        "Read [Deep Unordered Composition  Rivals Syntactic Methods for Text Classification by Iyyer et al. (2015)](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf)."
      ]
    },
    {
      "metadata": {
        "id": "mLD9C-5SyiDe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Processing\n",
        "\n",
        "Make sure you've downloaded the Stanford Sentiment Treebank that was used in lab. You can find it [here](http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip)."
      ]
    },
    {
      "metadata": {
        "id": "kFxv0DeYyiDj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import re\n",
        "import random\n",
        "\n",
        "random.seed(1)\n",
        "sst_home = ''\n",
        "\n",
        "# Let's do 2-way positive/negative classification instead of 5-way\n",
        "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
        "    # so labels of 0 and 1 in te 5-wayclassificaiton are 0 in the 2-way. 3 and 4 are 1, and 2 is none\n",
        "    # because we don't have a neautral class. \n",
        "\n",
        "PADDING = \"<PAD>\"\n",
        "UNKNOWN = \"<UNK>\"\n",
        "max_seq_length = 20\n",
        "\n",
        "def load_sst_data(path):\n",
        "    data = []\n",
        "    with open(path) as f:\n",
        "        for i, line in enumerate(f): \n",
        "            example = {}\n",
        "            example['label'] = easy_label_map[int(line[1])]\n",
        "            if example['label'] is None:\n",
        "                continue\n",
        "            \n",
        "            # Strip out the parse information and the phrase labels---we don't need those here\n",
        "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
        "            example['text'] = text[1:]\n",
        "            data.append(example)\n",
        "\n",
        "    random.seed(1)\n",
        "    random.shuffle(data)\n",
        "    return data\n",
        "     \n",
        "training_set = load_sst_data('train.txt')\n",
        "dev_set = load_sst_data('dev.txt')\n",
        "test_set = load_sst_data('test.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_gM0U6-tyiDo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "Next, we'll extract the vocabulary from the data, index each token, and finally convert the sentences into lists of indexed tokens. We are also padding and truncating all sentences to be of length=20. (Why? Think about how to handle batching. This is certainly not the only way! This is just simple.)"
      ]
    },
    {
      "metadata": {
        "id": "KsvsSTC7yiDp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "def tokenize(string):\n",
        "    return string.split()\n",
        "\n",
        "def build_dictionary(training_datasets):\n",
        "    \"\"\"\n",
        "    Extract vocabulary and build dictionary.\n",
        "    \"\"\"  \n",
        "    word_counter = collections.Counter()\n",
        "    for i, dataset in enumerate(training_datasets):\n",
        "        for example in dataset:\n",
        "            word_counter.update(tokenize(example['text']))\n",
        "        \n",
        "    vocabulary = set([word for word in word_counter])\n",
        "    vocabulary = list(vocabulary)\n",
        "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
        "        \n",
        "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "    return word_indices, len(vocabulary)\n",
        "\n",
        "def sentences_to_padded_index_sequences(word_indices, datasets):\n",
        "    \"\"\"\n",
        "    Annotate datasets with feature vectors. Adding right-sided padding. \n",
        "    \"\"\"\n",
        "    for i, dataset in enumerate(datasets):\n",
        "        for example in dataset:\n",
        "            example['text_index_sequence'] = torch.zeros(max_seq_length)\n",
        "\n",
        "            token_sequence = tokenize(example['text'])\n",
        "            padding = max_seq_length - len(token_sequence)\n",
        "\n",
        "            for i in range(max_seq_length):\n",
        "                if i >= len(token_sequence):\n",
        "                    index = word_indices[PADDING]\n",
        "                    pass\n",
        "                else:\n",
        "                    if token_sequence[i] in word_indices:\n",
        "                        index = word_indices[token_sequence[i]]\n",
        "                    else:\n",
        "                        index = word_indices[UNKNOWN]\n",
        "                example['text_index_sequence'][i] = index\n",
        "\n",
        "            example['text_index_sequence'] = example['text_index_sequence'].long().view(1,-1)\n",
        "            example['label'] = torch.LongTensor([example['label']])\n",
        "\n",
        "\n",
        "word_to_ix, vocab_size = build_dictionary([training_set])\n",
        "sentences_to_padded_index_sequences(word_to_ix, [training_set, dev_set, test_set])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-tOUka4_yiDv",
        "colab_type": "code",
        "outputId": "4819ea16-ea28-4b3d-9c4a-8de4845a2afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Size of training dataset:\", len(training_set))\n",
        "print(\"\\nFirst padded and indexified example in training data:\\n\", training_set[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Size of training dataset:', 6920)\n",
            "('\\nFirst padded and indexified example in training data:\\n', {'text': \"Resident Evil is n't a product of its cinematic predecessors so much as an MTV , sugar hysteria , and PlayStation cocktail .\", 'text_index_sequence': tensor([[14711,  1967,  8638,  9667,  6138,  5124, 13993,  2189,   431, 10746,\n",
            "          1200,   609, 11730,  1890, 15979,  8186,  8453, 16253,  8186, 14190]]), 'label': tensor([0])})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E6EC-7CjyiD0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "### Batichify data\n",
        "We're going to be doign mini-batch training. The following code makes data iterators and a batchifying function."
      ]
    },
    {
      "metadata": {
        "id": "IHWd5RwtyiD3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is the iterator we'll use during training. \n",
        "# It's a generator that gives you one batch at a time.\n",
        "def data_iter(source, batch_size):\n",
        "    dataset_size = len(source)\n",
        "    start = -1 * batch_size\n",
        "    order = list(range(dataset_size))\n",
        "    random.shuffle(order)\n",
        "\n",
        "    while True:\n",
        "        start += batch_size\n",
        "        if start > dataset_size - batch_size:\n",
        "            # Start another epoch.\n",
        "            start = 0\n",
        "            random.shuffle(order)   \n",
        "        batch_indices = order[start:start + batch_size]\n",
        "        batch = [source[index] for index in batch_indices]\n",
        "        yield [source[index] for index in batch_indices]\n",
        "\n",
        "# This is the iterator we use when we're evaluating our model. \n",
        "# It gives a list of batches that you can then iterate through.\n",
        "def eval_iter(source, batch_size):\n",
        "    batches = []\n",
        "    dataset_size = len(source)\n",
        "    start = -1 * batch_size\n",
        "    order = list(range(dataset_size))\n",
        "    random.shuffle(order)\n",
        "\n",
        "    while start < dataset_size - batch_size:\n",
        "        start += batch_size\n",
        "        batch_indices = order[start:start + batch_size]\n",
        "        batch = [source[index] for index in batch_indices]\n",
        "        if len(batch) == batch_size:\n",
        "            batches.append(batch)\n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "    return batches\n",
        "\n",
        "# The following function gives batches of vectors and labels, \n",
        "# these are the inputs to your model and loss function\n",
        "def get_batch(batch):\n",
        "    vectors = []\n",
        "    labels = []\n",
        "    for dict in batch:\n",
        "        vectors.append(dict[\"text_index_sequence\"])\n",
        "        labels.append(dict[\"label\"])\n",
        "    return vectors, labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g3F2bL1AyiEC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "We'll be looking at accuracy as our evaluation metric."
      ]
    },
    {
      "metadata": {
        "id": "ycYY2tm7yiEE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This function outputs the accuracy on the dataset, we will use it during training.\n",
        "def evaluate(model, data_iter):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(len(data_iter)):\n",
        "        vectors, labels = get_batch(data_iter[i])\n",
        "        vectors = torch.stack(vectors).squeeze()\n",
        "        labels = torch.stack(labels).squeeze()\n",
        "\n",
        "        output = model(vectors)\n",
        "        \n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "      \n",
        "    return correct / float(total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BIHV4VcuyiEJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## Training Loop\n",
        "\n",
        "The following function trains the model and reports model accuracy on the train and dev set every 5 epochs."
      ]
    },
    {
      "metadata": {
        "id": "T9URse7xyiEL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def training_loop(batch_size, num_epochs, model, loss_, optim, training_iter, dev_iter,\n",
        "                  train_eval_iter, verbose=True):\n",
        "    step = 0\n",
        "    epoch = 0\n",
        "    total_batches = int(len(training_set) / batch_size)\n",
        "    accuracies = []\n",
        "    while epoch <= num_epochs:\n",
        "        model.train()\n",
        "        vectors, labels = get_batch(next(training_iter)) \n",
        "        vectors = torch.stack(vectors).squeeze() # batch_size, seq_len\n",
        "        labels = torch.stack(labels).squeeze()\n",
        "        \n",
        "    \n",
        "        model.zero_grad()\n",
        "        \n",
        "        output = model(vectors)\n",
        "\n",
        "        lossy = loss_(output, labels)\n",
        "        lossy.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optim.step()\n",
        "        \n",
        "\n",
        "        if step % total_batches == 0:\n",
        "            model.eval()\n",
        "            if epoch % 5 == 0:\n",
        "                train_acc = evaluate(model, train_eval_iter)\n",
        "                eval_acc = evaluate(model, dev_iter)\n",
        "                accuracies.append(eval_acc)\n",
        "                if verbose:\n",
        "                    print(\"Epoch %i; Step %i; Loss %f; Train acc: %f; Dev acc %f\" \n",
        "                          %(epoch, step, lossy.item(),\\\n",
        "                            train_acc, eval_acc))\n",
        "            epoch += 1\n",
        "        step += 1\n",
        "    \n",
        "    best_dev = max(accuracies)\n",
        "    print(\"Best dev accuracy is {}\".format(best_dev))\n",
        "    return best_dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lF5QonxQyiES",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "# Part 1: Implement DAN (40 points)\n",
        "\n",
        "Following the [paper](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf), implement the Deep Averaging Network (DAN).\n",
        "\n",
        "Implementation details,\n",
        "- Instead of using \\code{tanh} activations however, use \\code{ReLU}. \n",
        "- Make the number of layers a variable, not a fixed value.\n",
        "- Make sure to implement word-dropout."
      ]
    },
    {
      "metadata": {
        "id": "9UP_X0DAyiEV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DAN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, \n",
        "                 batch_size, n_layers, drop_rate):\n",
        "        super(DAN, self).__init__()\n",
        "        \n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.embedding_size = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.batch_size = batch_size\n",
        "        self.n_layers = n_layers\n",
        "        self.drop_rate = drop_rate\n",
        "        \n",
        "        \"\"\"\n",
        "        YOUR CODE GOES HERE\n",
        "        \"\"\"\n",
        "        self.layers=nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(embedding_dim,hidden_size))\n",
        "        for i in range(2,self.n_layers):\n",
        "            self.append(nn.Linear(hidden_size.hidden_size))\n",
        "        self.layers.append(nn.Linear(hidden_size,output_size))\n",
        "        \n",
        "        #print(self.layers)\n",
        "        \n",
        "        self.init_weights()\n",
        "        \n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        YOUR CODE GOES HERE\n",
        "        \"\"\"\n",
        "        emb=self.embed(input)\n",
        "        output=emb.mean(1)\n",
        "        for i in range(self.n_layers-1):\n",
        "            output=nn.functional.relu(self.layers[i](output))\n",
        "            output=nn.functional.dropout(output,self.drop_rate)\n",
        "        output= self.layers[self.n_layers-1](output)\n",
        "        output= nn.functional.softmax(output)\n",
        "        return output\n",
        "\n",
        "    def init_hidden(self):\n",
        "        h0 = torch.zeros(self.batch_size, self.hidden_size)\n",
        "        return h0\n",
        "    \n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        #lin_layers = [self.l1, self.decoder]\n",
        "        #for i in range(1, self.n_layers):\n",
        "        #    lin_layers.append(getattr(self, 'l{}'.format(i)))\n",
        "        em_layer =  [self.embed]\n",
        "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
        "        lin_layers = self.layers\n",
        "        for layer in lin_layers:\n",
        "            layer.weight.data.uniform_(-initrange, initrange)\n",
        "            if layer in lin_layers:\n",
        "                layer.bias.data.fill_(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gnHQ8NZ4yiEb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the model!\n",
        "\n",
        "** Please use the hyperparameters and optimizer provided below. Do not make changes here. **"
      ]
    },
    {
      "metadata": {
        "id": "VCg17qm6yiEd",
        "colab_type": "code",
        "outputId": "a8d405e4-6972-42b0-ff9d-d7e2784aeb14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters \n",
        "input_size = vocab_size\n",
        "num_labels = 2\n",
        "hidden_dim = 24\n",
        "embedding_dim = 8\n",
        "batch_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.001\n",
        "drop_rate = 0.4\n",
        "num_epochs = 30\n",
        "\n",
        "\n",
        "# Build and initialize the model\n",
        "dan = DAN(vocab_size, embedding_dim, hidden_dim, num_labels, batch_size, num_layers, drop_rate)\n",
        "dan.init_weights()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
        "\n",
        "# Build data iterators\n",
        "training_iter = data_iter(training_set, batch_size)\n",
        "train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
        "dev_iter = eval_iter(dev_set[:500], batch_size)\n",
        "\n",
        "# Train the model\n",
        "training_loop(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0; Step 0; Loss 0.693138; Train acc: 0.554688; Dev acc 0.531250\n",
            "Epoch 5; Step 135; Loss 0.675479; Train acc: 0.675781; Dev acc 0.601562\n",
            "Epoch 10; Step 270; Loss 0.509216; Train acc: 0.847656; Dev acc 0.730469\n",
            "Epoch 15; Step 405; Loss 0.411843; Train acc: 0.937500; Dev acc 0.796875\n",
            "Epoch 20; Step 540; Loss 0.354379; Train acc: 0.953125; Dev acc 0.785156\n",
            "Epoch 25; Step 675; Loss 0.345872; Train acc: 0.972656; Dev acc 0.792969\n",
            "Epoch 30; Step 810; Loss 0.337218; Train acc: 0.984375; Dev acc 0.796875\n",
            "Best dev accuracy is 0.796875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.796875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "LQcmnL4HyiEo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "# Part 2: Hyperparameter tuning (40 points)\n",
        "\n",
        "Tune the DAN for learning rate, number of layers, and drop-out rate. Select a range for each parameter and then do a random search over these hyperparameters, trying a minimum 5 permutations of hyperparameters. Report results and the best hyperparameters you found. Do you see any patterns in your results?"
      ]
    },
    {
      "metadata": {
        "id": "oRr9Z9CRyiEq",
        "colab_type": "code",
        "outputId": "f78d786d-7979-48e0-e413-0fc23ec030e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "                \n",
        "# Hyperparameters 2\n",
        "input_size = vocab_size\n",
        "num_labels = 2\n",
        "hidden_dim = 24\n",
        "embedding_dim = 8\n",
        "batch_size = 256\n",
        "num_layers = 4\n",
        "learning_rate = 0.001\n",
        "drop_rate = 0.4\n",
        "num_epochs = 30\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
        "\n",
        "# Build data iterators\n",
        "training_iter = data_iter(training_set, batch_size)\n",
        "train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
        "dev_iter = eval_iter(dev_set[:500], batch_size)\n",
        "\n",
        "# Train the model\n",
        "training_loop(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0; Step 0; Loss 0.332679; Train acc: 0.980469; Dev acc 0.789062\n",
            "Epoch 5; Step 135; Loss 0.328873; Train acc: 0.980469; Dev acc 0.800781\n",
            "Epoch 10; Step 270; Loss 0.322728; Train acc: 0.984375; Dev acc 0.804688\n",
            "Epoch 15; Step 405; Loss 0.322801; Train acc: 0.988281; Dev acc 0.785156\n",
            "Epoch 20; Step 540; Loss 0.321557; Train acc: 0.992188; Dev acc 0.808594\n",
            "Epoch 25; Step 675; Loss 0.320005; Train acc: 0.992188; Dev acc 0.792969\n",
            "Epoch 30; Step 810; Loss 0.322944; Train acc: 0.992188; Dev acc 0.804688\n",
            "Best dev accuracy is 0.80859375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.80859375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "FyG3aBptguWh",
        "colab_type": "code",
        "outputId": "1dcd58c0-0f13-4bd2-dc92-d7316aba0a94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "                \n",
        "# Hyperparameters 3\n",
        "input_size = vocab_size\n",
        "num_labels = 2\n",
        "hidden_dim = 24\n",
        "embedding_dim = 8\n",
        "batch_size = 256\n",
        "num_layers = 4\n",
        "learning_rate = 0.0005\n",
        "drop_rate = 0.4\n",
        "num_epochs = 30\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
        "\n",
        "# Build data iterators\n",
        "training_iter = data_iter(training_set, batch_size)\n",
        "train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
        "dev_iter = eval_iter(dev_set[:500], batch_size)\n",
        "\n",
        "# Train the model\n",
        "training_loop(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0; Step 0; Loss 0.315523; Train acc: 0.996094; Dev acc 0.777344\n",
            "Epoch 5; Step 135; Loss 0.330741; Train acc: 0.996094; Dev acc 0.792969\n",
            "Epoch 10; Step 270; Loss 0.318331; Train acc: 0.996094; Dev acc 0.777344\n",
            "Epoch 15; Step 405; Loss 0.321516; Train acc: 0.996094; Dev acc 0.792969\n",
            "Epoch 20; Step 540; Loss 0.314178; Train acc: 0.996094; Dev acc 0.781250\n",
            "Epoch 25; Step 675; Loss 0.319081; Train acc: 0.996094; Dev acc 0.777344\n",
            "Epoch 30; Step 810; Loss 0.313781; Train acc: 0.996094; Dev acc 0.785156\n",
            "Best dev accuracy is 0.79296875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.79296875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "Dz0INfhsg0Z8",
        "colab_type": "code",
        "outputId": "5f084470-562a-4ea2-8a82-699fe2878887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "                \n",
        "# Hyperparameters 4\n",
        "input_size = vocab_size\n",
        "num_labels = 2\n",
        "hidden_dim = 24\n",
        "embedding_dim = 8\n",
        "batch_size = 256\n",
        "num_layers = 4\n",
        "learning_rate = 0.002\n",
        "drop_rate = 0.4\n",
        "num_epochs = 30\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
        "\n",
        "# Build data iterators\n",
        "training_iter = data_iter(training_set, batch_size)\n",
        "train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
        "dev_iter = eval_iter(dev_set[:500], batch_size)\n",
        "\n",
        "# Train the model\n",
        "training_loop(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0; Step 0; Loss 0.333162; Train acc: 0.992188; Dev acc 0.765625\n",
            "Epoch 5; Step 135; Loss 0.314002; Train acc: 0.992188; Dev acc 0.753906\n",
            "Epoch 10; Step 270; Loss 0.317312; Train acc: 0.996094; Dev acc 0.750000\n",
            "Epoch 15; Step 405; Loss 0.313887; Train acc: 0.996094; Dev acc 0.753906\n",
            "Epoch 20; Step 540; Loss 0.317189; Train acc: 0.996094; Dev acc 0.753906\n",
            "Epoch 25; Step 675; Loss 0.313786; Train acc: 0.996094; Dev acc 0.746094\n",
            "Epoch 30; Step 810; Loss 0.317177; Train acc: 0.996094; Dev acc 0.757812\n",
            "Best dev accuracy is 0.765625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.765625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "52AxrpTng2qk",
        "colab_type": "code",
        "outputId": "8dbf1fce-14e5-401a-ee6f-69c69f40ae21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "                \n",
        "# Hyperparameters 5\n",
        "input_size = vocab_size\n",
        "num_labels = 2\n",
        "hidden_dim = 24\n",
        "embedding_dim = 8\n",
        "batch_size = 256\n",
        "num_layers = 6\n",
        "learning_rate = 0.001\n",
        "drop_rate = 0.4\n",
        "num_epochs = 30\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
        "\n",
        "# Build data iterators\n",
        "training_iter = data_iter(training_set, batch_size)\n",
        "train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
        "dev_iter = eval_iter(dev_set[:500], batch_size)\n",
        "\n",
        "# Train the model\n",
        "training_loop(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0; Step 0; Loss 0.313531; Train acc: 0.996094; Dev acc 0.792969\n",
            "Epoch 5; Step 135; Loss 0.317298; Train acc: 0.996094; Dev acc 0.781250\n",
            "Epoch 10; Step 270; Loss 0.313275; Train acc: 1.000000; Dev acc 0.777344\n",
            "Epoch 15; Step 405; Loss 0.313273; Train acc: 1.000000; Dev acc 0.773438\n",
            "Epoch 20; Step 540; Loss 0.313366; Train acc: 1.000000; Dev acc 0.789062\n",
            "Epoch 25; Step 675; Loss 0.313273; Train acc: 1.000000; Dev acc 0.785156\n",
            "Epoch 30; Step 810; Loss 0.317177; Train acc: 1.000000; Dev acc 0.785156\n",
            "Best dev accuracy is 0.79296875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.79296875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "xWuVpwiKg2oO",
        "colab_type": "code",
        "outputId": "4abf75cc-2e2c-464a-d56a-b276303d8cec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "                \n",
        "# Hyperparameters 6\n",
        "input_size = vocab_size\n",
        "num_labels = 2\n",
        "hidden_dim = 24\n",
        "embedding_dim = 8\n",
        "batch_size = 256\n",
        "num_layers = 4\n",
        "learning_rate = 0.001\n",
        "drop_rate = 0.3\n",
        "num_epochs = 30\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
        "\n",
        "# Build data iterators\n",
        "training_iter = data_iter(training_set, batch_size)\n",
        "train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
        "dev_iter = eval_iter(dev_set[:500], batch_size)\n",
        "\n",
        "# Train the model\n",
        "training_loop(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0; Step 0; Loss 0.317224; Train acc: 1.000000; Dev acc 0.804688\n",
            "Epoch 5; Step 135; Loss 0.325070; Train acc: 1.000000; Dev acc 0.773438\n",
            "Epoch 10; Step 270; Loss 0.317171; Train acc: 1.000000; Dev acc 0.796875\n",
            "Epoch 15; Step 405; Loss 0.321182; Train acc: 1.000000; Dev acc 0.800781\n",
            "Epoch 20; Step 540; Loss 0.313265; Train acc: 1.000000; Dev acc 0.785156\n",
            "Epoch 25; Step 675; Loss 0.313305; Train acc: 1.000000; Dev acc 0.785156\n",
            "Epoch 30; Step 810; Loss 0.317214; Train acc: 1.000000; Dev acc 0.796875\n",
            "Best dev accuracy is 0.8046875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8046875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "1RoWiyUw3-re",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the best hypeparameters I used are: input_size = vocab_size\n",
        "num_labels = 2\n",
        "hidden_dim = 24\n",
        "embedding_dim = 8\n",
        "batch_size = 256\n",
        "num_layers = 4\n",
        "learning_rate = 0.001\n",
        "drop_rate = 0.4\n",
        "num_epochs = 30\n",
        "\n",
        "I found out that increasing number of layers to 4 can improve the performance of the model; however, further increasing it to 6 results in a lower accuracy. \n",
        "Increasing the learning rate to 0.002 seem to have a negative impact on the performance of the model. \n",
        "Changing the drop rate didn't seem to make a hugh difference in my case."
      ]
    },
    {
      "metadata": {
        "id": "VgD6cUxSyiE0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "# Part 3: short questions (20 points)"
      ]
    },
    {
      "metadata": {
        "id": "onihNYEwyiE1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Name, and briefly describe, 3 other possible composition functions, instead of the DAN, you could use to build sentence representations.\n",
        "2. Explain how dropout regularizes a model.\n",
        "3. What are the shortcomings for training for a fixed number of epochs? Give an alternative.\n",
        "4. Explain why you might use random search rather than grid search.\n",
        "\n",
        "_Bonus (5 points): briefly describe the Nelder–Mead method and how you might use it to do hyperparamter tuning. What are the tradeoffs between using Nelder-Mead vs random search?_"
      ]
    },
    {
      "metadata": {
        "id": "HZlsIeRnwdSJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "InferSent\n",
        "InferSent is a sentence embeddings method that provides semantic representations for English sentences. It is trained on natural language inference data and generalizes well to many different tasks.\n",
        "2.\n",
        "Skip-Thought Vectors\n",
        "In the skip-thought paper from 2015, the authors took the same intuition from the language model. However, in skip thought, instead of predicting the next word, we are predicting the previous and the next sentence. This gives the model more context for the sentence, thus we can build better representations of sentences. \n",
        "3.Learning Sentence Representations over Tree Structures for Target-dependent Classification\n",
        "a reinforcement learning based approach, which\n",
        "automatically induces target-specific sentence\n",
        "representations over tree structures. The underlying model is a RNN encoder-decoder that\n",
        "explores possible binary tree structures and a\n",
        "reward mechanism that encourages structures\n",
        "that improve performances on downstream\n",
        "tasks. \n"
      ]
    },
    {
      "metadata": {
        "id": "h1h9gu9-1yrd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2.Dropout randomly ignores selected neurons during training so that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. quoted partly from Jason Brownlee"
      ]
    },
    {
      "metadata": {
        "id": "nCxqbOiF2enS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3.The model might be able to continue improving after reaching the fixed number of epochs. An alternative: in Keras, there is an option of early stopping you can make when fitting a model to stop training once your validation accuracy flattens out, or starts to go back down"
      ]
    },
    {
      "metadata": {
        "id": "c45wNweP20vQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "4.As random values are selected at each instance, it is highly likely that the whole of action space has been reached because of the randomness, which takes a huge amount of time to cover every aspect of the combination during grid search.  quoted from Kishan Maladkar"
      ]
    },
    {
      "metadata": {
        "id": "nwnfZqiG2Dgh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "5.According to Wikipedia, \"The Nelder–Mead method (also downhill simplex method, amoeba method, or polytope method) is a commonly applied numerical method used to find the minimum or maximum of an objective function in a multidimensional space. It is a direct search method (based on function comparison) and is often applied to nonlinear optimization problems for which derivatives may not be known. \n",
        "Trade off: However, the Nelder–Mead technique is a heuristic search method that can converge to non-stationary points on problems that can be solved by alternative methods.\""
      ]
    },
    {
      "metadata": {
        "id": "yEKsKYV-yiE2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}