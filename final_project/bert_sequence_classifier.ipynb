{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of bert classifier.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"colab_type":"code","id":"wMMK-dTQTOJI","outputId":"2b8812e9-84dd-4c93-9f55-8ff70853a8dc","colab":{"base_uri":"https://localhost:8080/","height":404}},"cell_type":"code","source":["!pip install pytorch_pretrained_bert"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytorch_pretrained_bert in /Users/yusu/anaconda3/lib/python3.7/site-packages (0.6.1)\n","Requirement already satisfied: requests in /Users/yusu/anaconda3/lib/python3.7/site-packages (from pytorch_pretrained_bert) (2.21.0)\n","Requirement already satisfied: tqdm in /Users/yusu/anaconda3/lib/python3.7/site-packages (from pytorch_pretrained_bert) (4.28.1)\n","Requirement already satisfied: numpy in /Users/yusu/anaconda3/lib/python3.7/site-packages (from pytorch_pretrained_bert) (1.15.4)\n","Requirement already satisfied: boto3 in /Users/yusu/anaconda3/lib/python3.7/site-packages (from pytorch_pretrained_bert) (1.9.116)\n","Requirement already satisfied: torch>=0.4.1 in /Users/yusu/anaconda3/lib/python3.7/site-packages (from pytorch_pretrained_bert) (1.0.1.post2)\n","Requirement already satisfied: regex in /Users/yusu/anaconda3/lib/python3.7/site-packages (from pytorch_pretrained_bert) (2019.3.12)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/yusu/anaconda3/lib/python3.7/site-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /Users/yusu/anaconda3/lib/python3.7/site-packages (from requests->pytorch_pretrained_bert) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/yusu/anaconda3/lib/python3.7/site-packages (from requests->pytorch_pretrained_bert) (2018.11.29)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/yusu/anaconda3/lib/python3.7/site-packages (from requests->pytorch_pretrained_bert) (1.24.1)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.116 in /Users/yusu/anaconda3/lib/python3.7/site-packages (from boto3->pytorch_pretrained_bert) (1.12.116)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/yusu/anaconda3/lib/python3.7/site-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /Users/yusu/anaconda3/lib/python3.7/site-packages (from boto3->pytorch_pretrained_bert) (0.2.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/yusu/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.116->boto3->pytorch_pretrained_bert) (2.7.5)\n","Requirement already satisfied: docutils>=0.10 in /Users/yusu/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.116->boto3->pytorch_pretrained_bert) (0.14)\n","Requirement already satisfied: six>=1.5 in /Users/yusu/anaconda3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.116->boto3->pytorch_pretrained_bert) (1.12.0)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"cBHGVCGMTJcN","outputId":"a6c05c41-7283-441e-e60f-c284f804c19a","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import torch\n","from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM\n","version = \"bert-large-uncased\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"cnFXmBMmTJcY","colab":{}},"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained(version)\n","model = BertForMaskedLM.from_pretrained(version)\n","model.eval()\n","mask_tok = \"[MASK]\""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"TqbkC_X4TJds","colab":{}},"cell_type":"code","source":["data_dir = \"\"\n","max_seq_len = 128\n","batch_size = 32\n","n_epochs = 10\n","learning_rate = 1e-5\n","validate_every = 100\n","print_every = 500\n","n_labels = 2\n","n_train_steps = 200"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"6J9Cj2k2XzIt","colab":{}},"cell_type":"code","source":["from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\n","\n","# Prepare model\n","tokenizer = BertTokenizer.from_pretrained(version)\n","model = BertForSequenceClassification.from_pretrained(version, num_labels = n_labels)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Prepare optimizer\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","optimizer = BertAdam(optimizer_grouped_parameters, lr=learning_rate, warmup=0.1, t_total=n_train_steps)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"UqKHIkj0so6O"},"cell_type":"markdown","source":["### RUN THIS CODE BEFORE EXECUTING STUFF ###"]},{"metadata":{"colab_type":"code","id":"ZzqzdBBfH-IZ","colab":{}},"cell_type":"code","source":["data_dir=\"\""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"YOALkCIzTJdi","colab":{}},"cell_type":"code","source":["import os\n","import csv\n","import sys\n","import numpy as np\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","    def __init__(self, guid, text_a, text_b=None, label=None):\n","        \"\"\"Constructs a InputExample.\n","        Args:\n","            guid: Unique id for the example.\n","            text_a: string. The untokenized text of the first sequence. For single\n","            sequence tasks, only this sequence must be specified.\n","            text_b: (Optional) string. The untokenized text of the second sequence.\n","            Only must be specified for sequence pair tasks.\n","            label: (Optional) string. The label of the example. This should be\n","            specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.label = label\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_id = label_id\n","        \n","class DataProcessor(object):\n","    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._create_examples(\n","            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._create_examples(\n","            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"0\", \"1\"]\n","\n","    @classmethod\n","    def _read_tsv(cls, input_file, quotechar=None):\n","        \"\"\"Reads a tab separated value file.\"\"\"\n","        with open(input_file, \"r\") as f:\n","            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n","            lines = []\n","            for line in reader:\n","                if sys.version_info[0] == 2:\n","                    line = list(unicode(cell, 'utf-8') for cell in line)\n","                lines.append(line)\n","            return lines\n","        \n","class Sst2Processor(DataProcessor):\n","    \"\"\"Processor for the SST-2 data set (GLUE version).\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._create_examples(\n","            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._create_examples(\n","            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"0\", \"1\"]\n","\n","    def _create_examples(self, lines, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        examples = []\n","        for (i, line) in enumerate(lines):\n","            if i == 0:\n","                continue\n","            guid = \"%s-%s\" % (set_type, i)\n","            text_a = line[0]\n","            label = line[1]\n","            examples.append(\n","                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n","        return examples"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"fwoGLinRTJdm","colab":{}},"cell_type":"code","source":["def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n","    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n","\n","    label_map = {label : i for i, label in enumerate(label_list)}\n","\n","    features = []\n","    for (ex_index, example) in enumerate(examples):\n","        tokens_a = tokenizer.tokenize(example.text_a)\n","\n","        tokens_b = None\n","        if example.text_b:\n","            tokens_b = tokenizer.tokenize(example.text_b)\n","            # Modifies `tokens_a` and `tokens_b` in place so that the total\n","            # length is less than the specified length.\n","            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length )\n","        else:\n","            # Account for [CLS] and [SEP] with \"- 2\"\n","            if len(tokens_a) > max_seq_length :\n","                tokens_a = tokens_a[:(max_seq_length )]\n","\n","        # The convention in BERT is:\n","        # (a) For sequence pairs:\n","        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n","        # (b) For single sequences:\n","        #  tokens:   [CLS] the dog is hairy . [SEP]\n","        #  type_ids: 0   0   0   0  0     0 0\n","        #\n","        # Where \"type_ids\" are used to indicate whether this is the first\n","        # sequence or the second sequence. The embedding vectors for `type=0` and\n","        # `type=1` were learned during pre-training and are added to the wordpiece\n","        # embedding vector (and position vector). This is not *strictly* necessary\n","        # since the [SEP] token unambigiously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","        #\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        tokens =  tokens_a \n","        segment_ids = [0] * len(tokens)\n","\n","        if tokens_b:\n","            tokens += tokens_b \n","            segment_ids += [1] * (len(tokens_b) + 1)\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","        # tokens are attended to.\n","        input_mask = [1] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding = [0] * (max_seq_length - len(input_ids))\n","        input_ids += padding\n","        input_mask += padding\n","        segment_ids += padding\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","\n","        label_id = label_map[example.label]\n","        if ex_index < 5:\n","            print(\"*** Example ***\")\n","            print(\"guid: %s\" % (example.guid))\n","            print(\"tokens: %s\" % \" \".join(\n","                    [str(x) for x in tokens]))\n","            print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n","            print(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n","            print(\n","                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n","            print(\"label: %s (id = %d)\" % (example.label, label_id))\n","\n","        features.append(\n","                InputFeatures(input_ids=input_ids,\n","                              input_mask=input_mask,\n","                              segment_ids=segment_ids,\n","                              label_id=label_id))\n","    return features"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"hmK0ih5Maz_L","outputId":"01644db1-ad04-404b-fa65-bf393ed896d0","colab":{"base_uri":"https://localhost:8080/","height":1262}},"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","\n","\n","processor = Sst2Processor()\n","n_labels = 2\n","label_list = processor.get_labels()\n","\n","train_examples = processor.get_train_examples(data_dir)\n","train_features = convert_examples_to_features(train_examples, label_list, max_seq_len, tokenizer)\n","\n","all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n","all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n","all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n","all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n","train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","eval_examples = processor.get_dev_examples(data_dir)\n","eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_len, tokenizer)\n","all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n","all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n","all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n","all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n","eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","eval_sampler = SequentialSampler(eval_data)\n","eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)\n","\n","\n","n_train_steps = int(len(train_examples) / batch_size) * n_epochs"],"execution_count":0,"outputs":[{"output_type":"stream","text":["*** Example ***\n","guid: train-1\n","tokens: victim as a judge ruled the trial will ignore bullets found in his apartment the former new england patriot has denied shooting to death semi ##pro ##fe ##ssion ##al football player odin lloyd who was dating the sister of\n","input_ids: 6778 2004 1037 3648 5451 1996 3979 2097 8568 10432 2179 1999 2010 4545 1996 2280 2047 2563 16419 2038 6380 5008 2000 2331 4100 21572 7959 28231 2389 2374 2447 26195 6746 2040 2001 5306 1996 2905 1997 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","label: 1 (id = 1)\n","*** Example ***\n","guid: train-2\n","tokens: as a judge ruled the trial will ignore bullets found in his apartment the former new england patriot has denied shooting to death semi ##pro ##fe ##ssion ##al football player odin lloyd who was dating the sister of his\n","input_ids: 2004 1037 3648 5451 1996 3979 2097 8568 10432 2179 1999 2010 4545 1996 2280 2047 2563 16419 2038 6380 5008 2000 2331 4100 21572 7959 28231 2389 2374 2447 26195 6746 2040 2001 5306 1996 2905 1997 2010 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","label: 1 (id = 1)\n","*** Example ***\n","guid: train-3\n","tokens: ruled the trial will ignore bullets found in his apartment the former new england patriot has denied shooting to death semi ##pro ##fe ##ssion ##al football player odin lloyd who was dating the sister of his fiancee and today\n","input_ids: 5451 1996 3979 2097 8568 10432 2179 1999 2010 4545 1996 2280 2047 2563 16419 2038 6380 5008 2000 2331 4100 21572 7959 28231 2389 2374 2447 26195 6746 2040 2001 5306 1996 2905 1997 2010 19455 1998 2651 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","label: 0 (id = 0)\n","*** Example ***\n","guid: train-4\n","tokens: apartment which police claimed was the same caliber as the gun that killed lloyd because they had not provided a reason for a search warrant waiting for the news the did not seem worried as\n","input_ids: 4545 2029 2610 3555 2001 1996 2168 15977 2004 1996 3282 2008 2730 6746 2138 2027 2018 2025 3024 1037 3114 2005 1037 3945 10943 3403 2005 1996 2739 1996 2106 2025 4025 5191 2004 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","label: 0 (id = 0)\n","*** Example ***\n","guid: train-5\n","tokens: the gun that killed lloyd because they had not provided a reason for a search warrant waiting for the news the did not seem worried as his team persisted with their arguments and mr lloyd ##s\n","input_ids: 1996 3282 2008 2730 6746 2138 2027 2018 2025 3024 1037 3114 2005 1037 3945 10943 3403 2005 1996 2739 1996 2106 2025 4025 5191 2004 2010 2136 19035 2007 2037 9918 1998 2720 6746 2015 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","label: 0 (id = 0)\n","*** Example ***\n","guid: dev-1\n","tokens: victim as a judge ruled the trial will ignore bullets found in his apartment the former new england patriot has denied shooting to death semi ##pro ##fe ##ssion ##al football player odin lloyd who was dating the sister of\n","input_ids: 6778 2004 1037 3648 5451 1996 3979 2097 8568 10432 2179 1999 2010 4545 1996 2280 2047 2563 16419 2038 6380 5008 2000 2331 4100 21572 7959 28231 2389 2374 2447 26195 6746 2040 2001 5306 1996 2905 1997 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","label: 1 (id = 1)\n","*** Example ***\n","guid: dev-2\n","tokens: as a judge ruled the trial will ignore bullets found in his apartment the former new england patriot has denied shooting to death semi ##pro ##fe ##ssion ##al football player odin lloyd who was dating the sister of his\n","input_ids: 2004 1037 3648 5451 1996 3979 2097 8568 10432 2179 1999 2010 4545 1996 2280 2047 2563 16419 2038 6380 5008 2000 2331 4100 21572 7959 28231 2389 2374 2447 26195 6746 2040 2001 5306 1996 2905 1997 2010 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","label: 1 (id = 1)\n","*** Example ***\n","guid: dev-3\n","tokens: ruled the trial will ignore bullets found in his apartment the former new england patriot has denied shooting to death semi ##pro ##fe ##ssion ##al football player odin lloyd who was dating the sister of his fiancee and today\n","input_ids: 5451 1996 3979 2097 8568 10432 2179 1999 2010 4545 1996 2280 2047 2563 16419 2038 6380 5008 2000 2331 4100 21572 7959 28231 2389 2374 2447 26195 6746 2040 2001 5306 1996 2905 1997 2010 19455 1998 2651 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","label: 0 (id = 0)\n","*** Example ***\n","guid: dev-4\n","tokens: apartment which police claimed was the same caliber as the gun that killed lloyd because they had not provided a reason for a search warrant waiting for the news the did not seem worried as\n","input_ids: 4545 2029 2610 3555 2001 1996 2168 15977 2004 1996 3282 2008 2730 6746 2138 2027 2018 2025 3024 1037 3114 2005 1037 3945 10943 3403 2005 1996 2739 1996 2106 2025 4025 5191 2004 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","label: 0 (id = 0)\n","*** Example ***\n","guid: dev-5\n","tokens: the gun that killed lloyd because they had not provided a reason for a search warrant waiting for the news the did not seem worried as his team persisted with their arguments and mr lloyd ##s\n","input_ids: 1996 3282 2008 2730 6746 2138 2027 2018 2025 3024 1037 3114 2005 1037 3945 10943 3403 2005 1996 2739 1996 2106 2025 4025 5191 2004 2010 2136 19035 2007 2037 9918 1998 2720 6746 2015 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","label: 0 (id = 0)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"OqHZUJb4TJd8","colab":{}},"cell_type":"code","source":["def accuracy(out, labels):\n","    outputs = np.argmax(out, axis=1)\n","    return np.sum(outputs == labels)\n","\n","def evaluate(model, dataloader, batch_size=32):\n","    model.eval()\n","    eval_loss, eval_accuracy = 0., 0.\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n","        input_ids = input_ids.to(device)\n","        input_mask = input_mask.to(device)\n","        segment_ids = segment_ids.to(device)\n","        label_ids = label_ids.to(device)\n","\n","        with torch.no_grad():\n","            tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n","            logits = model(input_ids, segment_ids, input_mask)\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = label_ids.to('cpu').numpy()\n","        tmp_eval_accuracy = accuracy(logits, label_ids)\n","\n","        eval_loss += tmp_eval_loss.mean().item()\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        nb_eval_examples += input_ids.size(0)\n","        nb_eval_steps += 1\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_examples\n","    result = {'eval_loss': eval_loss,\n","              'eval_accuracy': eval_accuracy}\n","    return result"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"r6rAEV2UTJdy","outputId":"b0108b89-aaae-42be-dd2c-82b428f9f78e","colab":{"base_uri":"https://localhost:8080/","height":139}},"cell_type":"code","source":["import time\n","\n","max_n_steps = 200\n","\n","def train(model):\n","  print(\"***** Running training *****\")\n","  print(\"  Num examples = %d\" % len(train_examples))\n","  print(\"  Batch size = %d\" % batch_size)\n","  #print(\"  Num steps = %d\" % n_train_steps)\n","  global_step = 0\n","  nb_tr_steps = 0\n","  tr_loss = 0\n","  start_time = time.time()\n","  \n","  for epoch_n in range(1, int(n_epochs) + 1):\n","      print(\"Epoch %d\" % epoch_n)\n","      tr_loss = 0.\n","      nb_tr_examples, nb_tr_steps = 0, 0\n","      for step, batch in enumerate(train_dataloader):\n","          model.train()\n","          batch = tuple(t.to(device) for t in batch)\n","          input_ids, input_mask, segment_ids, label_ids = batch\n","          loss = model(input_ids, segment_ids, input_mask, label_ids)\n","          loss.backward()\n","\n","          tr_loss += loss.item()\n","          nb_tr_examples += input_ids.size(0)\n","          nb_tr_steps += 1\n","          optimizer.step()\n","          optimizer.zero_grad()\n","          global_step += 1\n","\n","          if (step + 1) % print_every == 0:\n","              print(\"\\tStep %d:\\ttrain loss %.3f\" % (step + 1, tr_loss / nb_tr_examples))\n","              print(\"\\t\\tTrained on %d examples in %.3f\" % (nb_tr_examples, time.time() - start_time))\n","              start_time = time.time()\n","          if (step + 1) % validate_every == 0:\n","              print(\"\\tValidating...\")\n","              results = evaluate(model, eval_dataloader, batch_size=batch_size)\n","              print(\"\\t\\tdev accuracy: %.3f\" % results[\"eval_accuracy\"])\n","\n","          if step >= max_n_steps:\n","              return\n","\n","train(model)\n","results = evaluate(model, eval_dataloader, batch_size=batch_size)\n","print(\"Final dev acc: %.3f\" % results[\"eval_accuracy\"])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["***** Running training *****\n","  Num examples = 10\n","  Batch size = 32\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Final dev acc: 0.778\n"],"name":"stdout"}]},{"metadata":{"id":"jRkGcj2uVZJU","colab_type":"code","colab":{}},"cell_type":"code","source":["from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S4_XVTUkVZJW","colab_type":"code","colab":{}},"cell_type":"code","source":["#here is how to save model\n","model_save_dir='/Users/yusu/Downloads/'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QCv4NKyLVZJZ","colab_type":"code","colab":{}},"cell_type":"code","source":["model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","output_model_file = os.path.join(model_save_dir, WEIGHTS_NAME)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"huH76KowVZJb","colab_type":"code","colab":{}},"cell_type":"code","source":["torch.save(model_to_save.state_dict(), output_model_file)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qg1wUzDYVZJi","colab_type":"code","colab":{}},"cell_type":"code","source":["config_save_dir='/Users/yusu/Downloads/'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cnTaHffMVZJq","colab_type":"code","colab":{}},"cell_type":"code","source":["output_config_file = os.path.join(config_save_dir, CONFIG_NAME)\n","with open(output_config_file, 'w') as f:\n","    f.write(model_to_save.config.to_json_string())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DlKjYUAEVZJu","colab_type":"code","colab":{}},"cell_type":"code","source":["# Load a trained model and config that you have fine-tuned\n","config = BertConfig(output_config_file)\n","model = BertForSequenceClassification(config, num_labels=2)\n","model.load_state_dict(torch.load(output_model_file))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T8HN_Y8xVZJ0","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}