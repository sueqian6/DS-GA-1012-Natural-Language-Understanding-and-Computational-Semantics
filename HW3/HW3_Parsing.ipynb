{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "PNBPECURjIkk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Homework 3, DS-GA 1012, Spring 2019\n",
        "\n",
        "## Due April 3rd at 2pm (ET)\n",
        "\n",
        "Use the Stanford NLP parser (http://nlp.stanford.edu:8080/parser/) to complete the following exercises. Do not re-use examples from lab, but feel free to take inspiration from them.\n",
        "If you want a direct visualization of dependency structures, go to http://nlp.stanford.edu:8080/corenlp/."
      ]
    },
    {
      "metadata": {
        "id": "xE2u9ZofjIkm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Recall the [UD](http://universaldependencies.org/u/dep/) and [PTB](http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html) types. Give a set of sentences that show at least 15 different Universal Dependency relation types and 15 different PTB _phrase_ types. List the relation and phrase types included in your sentences. [15 pts]"
      ]
    },
    {
      "metadata": {
        "id": "pB6Tsh5_rTA5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1) The Stanford typed dependencies representation was designed to provide a simple description of the\n",
        "grammatical relationships in a sentence that can easily be understood and effectively used by people\n",
        "without linguistic expertise who want to extract textual relations. \n",
        "\n",
        "UD:\n",
        "('The', '5', 'det')\n",
        "('Stanford', '3', 'compound')\n",
        "('typed', '5', 'amod')\n",
        "('dependencies', '5', 'compound')\n",
        "('representation', '7', 'nsubj:pass')\n",
        "('was', '7', 'aux:pass')\n",
        "('designed', '0', 'root')\n",
        "('to', '9', 'mark')\n",
        "('provide', '7', 'advcl')\n",
        "('a', '12', 'det')\n",
        "('simple', '12', 'amod')\n",
        "('description', '9', 'obj')\n",
        "('of', '16', 'case')\n",
        "('the', '16', 'det')\n",
        "('grammatical', '16', 'amod')\n",
        "('relationships', '12', 'nmod')\n",
        "('in', '19', 'case')\n",
        "('a', '19', 'det')\n",
        "('sentence', '16', 'nmod')\n",
        "('that', '24', 'nsubj:pass')\n",
        "('can', '24', 'aux')\n",
        "('easily', '24', 'advmod')\n",
        "('be', '24', 'aux:pass')\n",
        "('understood', '19', 'acl:relcl')\n",
        "('and', '27', 'cc')\n",
        "('effectively', '27', 'advmod')\n",
        "('used', '24', 'conj')\n",
        "('by', '29', 'case')\n",
        "('people', '27', 'obl')\n",
        "('without', '32', 'case')\n",
        "('linguistic', '32', 'amod')\n",
        "('expertise', '29', 'nmod')\n",
        "('who', '34', 'nsubj')\n",
        "('want', '32', 'acl:relcl')\n",
        "('to', '36', 'mark')\n",
        "('extract', '34', 'xcomp')\n",
        "('textual', '38', 'amod')\n",
        "('relations', '36', 'obj')\n",
        "('.', '7', 'punct')\n",
        "\n",
        "PTB:(NNP, JJ, NNS, NN, VBD, TO, VB, DT, IN, WDT, MD, RB, VBP, NNS, WP)\n",
        "\n",
        "(ROOT\n",
        "  (S\n",
        "    (NP (DT The) (NNP Stanford) (JJ typed) (NNS dependencies) (NN representation))\n",
        "    (VP (VBD was)\n",
        "      (VP (VBN designed)\n",
        "        (S\n",
        "          (VP (TO to)\n",
        "            (VP (VB provide)\n",
        "              (NP\n",
        "                (NP (DT a) (JJ simple) (NN description))\n",
        "                (PP (IN of)\n",
        "                  (NP (DT the) (JJ grammatical) (NNS relationships))))\n",
        "              (PP (IN in)\n",
        "                (NP\n",
        "                  (NP (DT a) (NN sentence))\n",
        "                  (SBAR\n",
        "                    (WHNP (WDT that))\n",
        "                    (S\n",
        "                      (VP (MD can)\n",
        "                        (ADVP (RB easily))\n",
        "                        (VP (VB be)\n",
        "                          (VP\n",
        "                            (VP (VBN understood))\n",
        "                            (CC and)\n",
        "                            (VP\n",
        "                              (ADVP (RB effectively))\n",
        "                              (VBN used)\n",
        "                              (PP (IN by)\n",
        "                                (NP (NNS people))))\n",
        "                            (PP (IN without)\n",
        "                              (NP\n",
        "                                (NP (JJ linguistic) (NN expertise))\n",
        "                                (SBAR\n",
        "                                  (WHNP (WP who))\n",
        "                                  (S\n",
        "                                    (VP (VBP want)\n",
        "                                      (S\n",
        "                                        (VP (TO to)\n",
        "                                          (VP (VB extract)\n",
        "                                            (NP (JJ textual) (NNS relations))))))))))))))))))))))\n",
        "    (. .)))\n",
        "    \n",
        "    2) The major English dependency treebanks have largely been extracted from existing resources such as the Wall Street Journal sections of the Penn Treebank.\n",
        "    \n",
        "    PTB: (NNP, VBD, IN, DT, JJ, NN, POS, RB, VB, VBN, PRP, CC, VBG, NNP, CD)\n",
        " (ROOT\n",
        "  (S\n",
        "    (NP (NNP Attorney) (NNP General) (NNP William) (NNP Barr))\n",
        "    (VP (VBD wrote)\n",
        "      (SBAR (IN that)\n",
        "        (S\n",
        "          (NP\n",
        "            (NP (DT the) (JJ special) (NN counsel) (POS 's))\n",
        "            (JJ final) (NN report))\n",
        "          (VP (VBD did) (RB not)\n",
        "            (VP (VB find)\n",
        "              (SBAR (IN that)\n",
        "                (S\n",
        "                  (NP\n",
        "                    (NP (NNP Trump))\n",
        "                    (CC or)\n",
        "                    (NP (PRP$ his) (NN campaign)))\n",
        "                  (VP\n",
        "                    (VP (VBD had) (`` ``)\n",
        "                      (VP (VBN conspired)\n",
        "                        (CC or)\n",
        "                        (VBN coordinated) ('' '')\n",
        "                        (PP (IN with)\n",
        "                          (NP\n",
        "                            (NP (NNP Russia))\n",
        "                            (PP (IN during)\n",
        "                              (NP (DT the) (CD 2016) (NN election)))))))\n",
        "                    (, ,)\n",
        "                    (CC and)\n",
        "                    (VP (VBD did) (RB not)\n",
        "                      (VP (VB reach)\n",
        "                        (NP (DT a) (NN conclusion))\n",
        "                        (S\n",
        "                          (VP\n",
        "                            (VP (VBG regarding)\n",
        "                              (NP\n",
        "                                (NP (NN obstruction))\n",
        "                                (PP (IN of)\n",
        "                                  (NP (NN justice)))))\n",
        "                            (, ,)\n",
        "                            (CC neither)\n",
        "                            (VP (VBG implicating)\n",
        "                              (S\n",
        "                                (NP (PRP him))\n",
        "                                (VP\n",
        "                                  (VP (VBG regarding)\n",
        "                                    (NP\n",
        "                                      (NP (NN obstruction))\n",
        "                                      (PP (IN of)\n",
        "                                        (NP (NN justice)))))\n",
        "                                  (CC nor)\n",
        "                                  (VP (VBG exonerating)\n",
        "                                    (NP (PRP him))))))))))))))))))\n",
        "    (. .)))\n",
        "    \n",
        "Universal dependencies\n",
        "compound(Barr-4, Attorney-1)\n",
        "compound(Barr-4, General-2)\n",
        "compound(Barr-4, William-3)\n",
        "nsubj(wrote-5, Barr-4)\n",
        "root(ROOT-0, wrote-5)\n",
        "mark(find-15, that-6)\n",
        "det(counsel-9, the-7)\n",
        "amod(counsel-9, special-8)\n",
        "nmod:poss(report-12, counsel-9)\n",
        "case(counsel-9, 's-10)\n",
        "amod(report-12, final-11)\n",
        "nsubj(find-15, report-12)\n",
        "aux(find-15, did-13)\n",
        "neg(find-15, not-14)\n",
        "ccomp(wrote-5, find-15)\n",
        "mark(conspired-23, that-16)\n",
        "nsubj(conspired-23, Trump-17)\n",
        "cc(Trump-17, or-18)\n",
        "nmod:poss(campaign-20, his-19)\n",
        "conj(Trump-17, campaign-20)\n",
        "aux(conspired-23, had-21)\n",
        "ccomp(find-15, conspired-23)\n",
        "cc(conspired-23, or-24)\n",
        "conj(conspired-23, coordinated-25)\n",
        "case(Russia-28, with-27)\n",
        "conj(conspired-23, Russia-28)\n",
        "case(election-32, during-29)\n",
        "det(election-32, the-30)\n",
        "nummod(election-32, 2016-31)\n",
        "nmod(Russia-28, election-32)\n",
        "cc(conspired-23, and-34)\n",
        "aux(reach-37, did-35)\n",
        "neg(reach-37, not-36)\n",
        "conj(conspired-23, reach-37)\n",
        "det(conclusion-39, a-38)\n",
        "dobj(reach-37, conclusion-39)\n",
        "dep(reach-37, regarding-40)\n",
        "dobj(regarding-40, obstruction-41)\n",
        "case(justice-43, of-42)\n",
        "nmod(obstruction-41, justice-43)\n",
        "dep(regarding-40, neither-45)\n",
        "conj(regarding-40, implicating-46)\n",
        "nsubj(regarding-48, him-47)\n",
        "ccomp(implicating-46, regarding-48)\n",
        "dobj(regarding-48, obstruction-49)\n",
        "case(justice-51, of-50)\n",
        "nmod(obstruction-49, justice-51)\n",
        "cc(regarding-48, nor-52)\n",
        "conj(regarding-48, exonerating-53)\n",
        "dobj(exonerating-53, him-54)"
      ]
    },
    {
      "metadata": {
        "id": "OPjzoaWLjIko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. Give a sentence with a dependency between two words that spans more than 10 words. [5 pts]\n",
        "\n",
        "The girl that we saw in Natural Language Understanding and Computational Semantics class last week came to the party yesterday. ( \"girl\" and \"came\" )"
      ]
    },
    {
      "metadata": {
        "id": "WuGLPbFrjIkp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. Give a sentence of at least six words for which all of the words depend on the main verb. [5 pts]\n",
        "\n",
        "I can never sing aloud bravely. \n",
        "\n",
        "![替代文字](https://github.com/sueqian6/DS-GA-1012-Natural-Language-Understanding-and-Computational-Semantics/blob/master/HW3/31.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "LNiiG9HcjIkq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "4. Give a word that can appear with at least two different parts of speech (e.g. noun and verb, noun and adjective), and a pair of sentences using that word with different parts of speech. [5 pts]\n",
        "\n",
        "I would like to dance with you. (DANCE: VB)\n",
        "\n",
        "![替代文字](https://github.com/sueqian6/DS-GA-1012-Natural-Language-Understanding-and-Computational-Semantics/blob/master/HW3/41.png?raw=true)\n",
        "\n",
        "I would like to go to the dance with you. (DANCE: NN)\n",
        "\n",
        "![替代文字](https://github.com/sueqian6/DS-GA-1012-Natural-Language-Understanding-and-Computational-Semantics/blob/master/HW3/42.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "VLmLvY9ojIkr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "5. Run the sentences “Who saw you?” and “Who did you see?” through the parser. Look at the PTB vs. dependency parses: what are the relative strengths and weaknesses of the two parsing grammars in parsing this pair of sentences? [10 pts]\n",
        "\n",
        "\"who saw you?\"\n",
        "ptb:\n",
        "(ROOT\n",
        "  (SBARQ\n",
        "    (WHNP (WP Who))\n",
        "    (SQ\n",
        "      (VP (VBD saw)\n",
        "        (NP (PRP you))))\n",
        "    (. ?)))\n",
        "    \n",
        "dependency parse:\n",
        "nsubj(saw-2, Who-1)\n",
        "root(ROOT-0, saw-2)\n",
        "dobj(saw-2, you-3)\n",
        "\n",
        "\"who did you see?\"\n",
        "ptb:\n",
        "(ROOT\n",
        "  (SBARQ\n",
        "    (WHNP (WP Who))\n",
        "    (SQ (VBD did)\n",
        "      (NP (PRP you))\n",
        "      (VP (VB see)))\n",
        "    (. ?)))\n",
        "    \n",
        "dependencies parse:\n",
        "dobj(see-4, Who-1)\n",
        "aux(see-4, did-2)\n",
        "nsubj(see-4, you-3)\n",
        "root(ROOT-0, see-4)\n",
        "\n",
        "Dependency parsing is better at dealing with a flexible order of words. PTB is better at describing relations between different parts."
      ]
    },
    {
      "metadata": {
        "id": "7aZVjjAUjIks",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "6. Look at the dependency and PTB representations for an active sentence and its corresponding passive sentence. e.g. `The dog ate the pie.` or `The pie was eaten by the dog.`. Do either of these parses give hints about the semantic link between active/passive pairs? [5 pts]\n",
        "\n",
        "The dog ate the pie.\n",
        "Parse\n",
        "(ROOT\n",
        "  (S\n",
        "    (NP (DT The) (NN dog))\n",
        "    (VP (VBD ate)\n",
        "      (NP (DT the) (NN pie)))\n",
        "    (. .)))\n",
        "Universal dependencies\n",
        "det(dog-2, The-1)\n",
        "nsubj(ate-3, dog-2)\n",
        "root(ROOT-0, ate-3)\n",
        "det(pie-5, the-4)\n",
        "dobj(ate-3, pie-5)\n",
        "\n",
        "The pie was eaten by the dog.\n",
        "Parse\n",
        "(ROOT\n",
        "  (S\n",
        "    (NP (DT The) (NN pie))\n",
        "    (VP (VBD was)\n",
        "      (VP (VBN eaten)\n",
        "        (PP (IN by)\n",
        "          (NP (DT the) (NN dog)))))\n",
        "    (. .)))\n",
        "Universal dependencies\n",
        "det(pie-2, The-1)\n",
        "nsubjpass(eaten-4, pie-2)\n",
        "auxpass(eaten-4, was-3)\n",
        "root(ROOT-0, eaten-4)\n",
        "case(dog-7, by-5)\n",
        "det(dog-7, the-6)\n",
        "nmod(eaten-4, dog-7)\n",
        "\n",
        "PTB doesn't but DP does."
      ]
    },
    {
      "metadata": {
        "id": "TYxFtxM7jIkt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "7. Discuss how one of these parses could be modified to better capture this link. Hint: look at the enhanced dependency parse for these sentences in the visualizer and think about semantic roles, e.g. “agent”. [10 pts]\n",
        "\n",
        "Maybe labeling every argument with a semantic role ( for example, agent and patient ) to disambiguate is a good idea. In the visualization of the enhanced parser, \"dog\" was labeled with \"agent\"."
      ]
    },
    {
      "metadata": {
        "id": "_wZiUj_sjIku",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "8. Give a rule (informally, in English) that will allow you to use a dependency parse to detect number agreement errors (i.e., The cat eat fish.) [10 pts]\n",
        "\n",
        "For example, in this case \"cat\" is annotated as NN, meaning it's singular or mass. However, \"eat\" is annotated as VBP, meaning \"Verb, non-3rd person singular present\". It's in conflict.\n",
        "A rule like this should be added: if NN or NNP is paired with VBZ, or if NNS or NNPS is paired with VBP, number agreement is corect. If NN or NNP is paired with VBP, or if NNS or NNPS is paired with VBZ. number agreement errors are detected.\n"
      ]
    },
    {
      "metadata": {
        "id": "GUUynHv_jIkv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "9. Consider the famous ambiguous sentence “I saw the man with the telescope”. Capture the semantic ambiguity by building a PTB tree structure and a dependency structure for both meanings (4 structures altogether). Make sure to label all arcs in the dependency structure, and all nodes in the PTB tree structure. You can use bracketing like in the parser, or draw them using LaTeX (you can use tikz-qtree and tikz-dependency), or neatly by hand and take a picture. [20 pts]\n",
        "![替代文字](https://github.com/sueqian6/DS-GA-1012-Natural-Language-Understanding-and-Computational-Semantics/blob/master/HW3/9.jpg?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "nPZDQ5RVjIkw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "10. Consider a model trained on data annotated with dependencies or PTB structure (like the online parsers you’ve been using), and an NLP task (e.g. translation, NLI). Would the model do better or worse on that task than a model trained on data with no syntactic annotation? Why? (no right answer here, just give at least one argument for your reasoning) [5 pts]\n",
        "\n",
        "I think the model will do better. For example, when swapping gendered words in a text, sometimes \"his\" should be swapped into \"her\" and sometimes \"hers\". With annotations, the model can tell the difference between the two cases."
      ]
    },
    {
      "metadata": {
        "id": "ZBk_dJgUjIkx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "11. Thought experiment: Discuss which features you could extract from dependency and/or constituency parses to use in the sentiment classifier models we have studied thus far. [10 pts]\n",
        "\n",
        "When words are annotated as \"neg\" (negation), such as \"not\", the sentiment classifier models should view the negation word and the adjective word as a combination, instead of treating them seperately."
      ]
    }
  ]
}
