{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1_yq729.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "fbjZ1BNrG6DY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Homework 1, DS-GA 1012, Spring 2019\n",
        "\n",
        "## Due Feburary 13, 2019 at 2pm (ET)\n",
        "\n",
        "Download the data zip `DS-GA1012-hw1-data.zip`. Complete the following questions in the notebook and submit your completed notebook on NYU Classes."
      ]
    },
    {
      "metadata": {
        "id": "duzx9jcALYgM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## set environment"
      ]
    },
    {
      "metadata": {
        "id": "wTJ0wvZXLWvO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from numpy.linalg import svd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SXizdE34LYPJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tqdtf-3-G6Da",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Exploring effect of context size [30 pts]"
      ]
    },
    {
      "metadata": {
        "id": "NVIXVaAlG6Db",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We face many implicit and explicit design decisions in creating distributional word representations. For example, in lecture and in lab, we created word vectors using a co-occurence matrix built on neighboring pairs of words. We might suspect, however, that we can get more signal of word similarity by considering larger contexts than pairs of words."
      ]
    },
    {
      "metadata": {
        "id": "jyun_MyQG6Dc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__a__. Write `build_cooccurrence_matrix`, which generates the co-occurence matrix for a window of arbitrary size and for the vocabulary of `max_vocab_size` most frequent words. Feel free to modify the code used in lab [10 pts]"
      ]
    },
    {
      "metadata": {
        "id": "fbQyQzuvITHI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Yusu's homework\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "def build_cooccurrence_matrix(data, max_vocab_size=20000,context_size=1):\n",
        "    \"\"\" \n",
        "    \n",
        "    args:\n",
        "        - data: iterable where each item is a string sentence\n",
        "        - max_vocab_size: maximum vocabulary size\n",
        "        \n",
        "    returns:\n",
        "        - coocur_mat: co-occurrence matrix as a numpy array\n",
        "    \"\"\"\n",
        "    \n",
        "    def get_token_frequencies():\n",
        "        tok2freq = defaultdict(int)\n",
        "        coocur_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "        for datum in data:\n",
        "            tokens = datum.strip().split() # we'll use whitespace to tokenize\n",
        "            for i, tok in enumerate(tokens):\n",
        "                tok2freq[tok] += 1\n",
        "                coocur_counts[tok][tok] += 1\n",
        "                if i < len(tokens) - 1:\n",
        "                    coocur_counts[tok][tokens[i+1]] += 1\n",
        "                    coocur_counts[tokens[i+1]][tok] += 1\n",
        "        return tok2freq, coocur_counts\n",
        "    \n",
        "    def prune_vocabulary(tok2freq, max_vocab_size):\n",
        "        \"\"\" Prune vocab by taking max_vocab_size most frequent words \"\"\"\n",
        "        tok_and_freqs = [(k, v) for k, v in tok2freq.items()]\n",
        "        tok_and_freqs.sort(key = lambda x: x[1], reverse=True) # sorts in-place\n",
        "        tok2idx = {tok: idx for idx, (tok, _) in enumerate(tok_and_freqs[:max_vocab_size])}\n",
        "        idx2tok = {idx: tok for tok, idx in tok2idx.items()}\n",
        "        return tok2idx, idx2tok\n",
        "    \n",
        "    def _build_coocurrence_mat(idx2tok, coocur_counts):\n",
        "        mat = [[coocur_counts[idx2tok[i]][idx2tok[j]] for j in range(len(idx2tok))] for i in range(len(idx2tok))]\n",
        "        \"\"\"vocab_size = len(idx2tok)\n",
        "        mat = [[0 for _ in range(vocab_size)] for _ in range(vocab_size)]\n",
        "        for i in range(vocab_size - context_size):\n",
        "            for j in range(i+context_size, vocab_size):\n",
        "                if coocur_counts[idx2tok[i]][idx2tok[j]]:\n",
        "                    mat[i][j] = coocur_counts[idx2tok[i]][idx2tok[j]]\n",
        "                    mat[j][i] = coocur_counts[idx2tok[i]][idx2tok[j]]\"\"\"\n",
        "        return np.array(mat)\n",
        "        \n",
        "    print(\"Counting words...\")\n",
        "    start_time = time.time()\n",
        "    tok2freq, coocur_counts = get_token_frequencies()\n",
        "    print(\"\\tFinished counting words in %.5f\" % (time.time() - start_time))\n",
        "\n",
        "    print(\"Pruning vocabulary...\")\n",
        "    tok2idx, idx2tok = prune_vocabulary(tok2freq, max_vocab_size)\n",
        "    start_time = time.time()\n",
        "    print(\"\\tFinished pruning vocabulary in %.5f\" % (time.time() - start_time))\n",
        "    \n",
        "    print(\"Building co-occurrence matrix...\")\n",
        "    start_time = time.time()\n",
        "    coocur_mat = _build_coocurrence_mat(idx2tok, coocur_counts)\n",
        "    print(\"\\tFinished building co-occurrence matrix in %.5f\" % (time.time() - start_time))\n",
        "    return coocur_mat, tok2idx, idx2tok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wy9ewGzpG6Dh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use your implementation of `build_cooccurrence_matrix` to generate the co-occurence matrix from the sentences of [SST](http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip) (file `datasetSentences.txt`) with `context_size=2` and `max_vocab_size=10000`. What is the co-occurrence count of the words \"the\" and \"end\"? "
      ]
    },
    {
      "metadata": {
        "id": "UEJfhOpTLKLB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_sst(data_file):\n",
        "    with open(data_file, 'r') as data_fh:\n",
        "        data_fh.readline() # skip the header\n",
        "        data = [r.split('\\t')[1] for r in data_fh.readlines()]\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ak-zG5C-JhQS",
        "colab_type": "code",
        "outputId": "1ee1062b-15dc-44a0-95d1-6214e7f7cf68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "data_file = 'datasetSentences.txt'\n",
        "\n",
        "data = load_sst(data_file)\n",
        "mat, tok2idx, idx2tok = build_cooccurrence_matrix(data, max_vocab_size=10000,context_size=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting words...\n",
            "\tFinished counting words in 0.41888\n",
            "Pruning vocabulary...\n",
            "\tFinished pruning vocabulary in 0.00000\n",
            "Building co-occurrence matrix...\n",
            "\tFinished building co-occurrence matrix in 54.01308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_wTk5sx831_M",
        "colab_type": "code",
        "outputId": "ee660a58-6263-4e5f-945f-af632827575e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "mat"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[11162,     0,     0, ...,     0,     0,     1],\n",
              "       [    0,  9949,   466, ...,     0,     0,     0],\n",
              "       [    0,   466,  8395, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,     2,     0,     0],\n",
              "       [    0,     0,     0, ...,     0,     2,     0],\n",
              "       [    1,     0,     0, ...,     0,     0,     2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "nSKXkXKj5GiC",
        "colab_type": "code",
        "outputId": "908c0eb7-922b-4bf7-f44a-6fe51cf81f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#find the index of 'the'\n",
        "list(tok2idx.keys()).index(\"the\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "LTDyFBa-8WNx",
        "colab_type": "code",
        "outputId": "97d19e1f-5e88-4b8c-b50a-f4de62cb3a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#find the index of 'and'\n",
        "list(tok2idx.keys()).index(\"and\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "9rFgot2H8rr1",
        "colab_type": "code",
        "outputId": "0ed1eeb3-2ae7-4817-e1b3-38d0b6dc9ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#find the cooccurence of 'the' and 'and'\n",
        "mat[2][3]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "348"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "Tj9NdNGf98id",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#save for later\n",
        "mat_1=mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PqDDk6QUG6Di",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__b__. Plot the effect of varying context size in $\\{1, 2, 3, 4\\}$ (leaving all the other settings the same) on the quality of the learned word embeddings, as measured by performance (Spearman correlation) on the word similarity dataset [MTurk-771](http://www2.mta.ac.il/~gideon/mturk771.html) between human judgments and cosine similarity of the learned word vectors (see lab). [12 pts]"
      ]
    },
    {
      "metadata": {
        "id": "7DacsJEJMj9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evaluation: Word Similarity"
      ]
    },
    {
      "metadata": {
        "id": "bWj8SrIjQuzT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QatS_rcFQrLH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_word_similarity_dataset(data_file):\n",
        "    with open(data_file, 'r') as data_fh:\n",
        "        raw_data = data_fh.readlines()\n",
        "    data = []\n",
        "    trgs = []\n",
        "    for datum in raw_data:\n",
        "        datum = datum.strip().split(',')\n",
        "        data.append((datum[0], datum[1]))\n",
        "        trgs.append(float(datum[2]))\n",
        "    return data, trgs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M-MT7_7UMtNr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_file = 'MTURK-771.csv'\n",
        "test_data, test_trgs = load_word_similarity_dataset(test_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hh8I9xF2N4lU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we evaluate when context_size=1."
      ]
    },
    {
      "metadata": {
        "id": "7nfGMcpePAd0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_word_similarity(word_pairs, targets, mat, tok2idx):\n",
        "    \"\"\" \"\"\"\n",
        "    preds = []\n",
        "    trgs = []\n",
        "    n_exs = 0\n",
        "    for (word1, word2), trg in zip(word_pairs, targets):\n",
        "        if word1 in tok2idx and word2 in tok2idx:\n",
        "            pred_sim = 1 - cosine(mat[tok2idx[word1]], mat[tok2idx[word2]])\n",
        "            preds.append(pred_sim)\n",
        "            trgs.append(trg)\n",
        "            n_exs += 1\n",
        "    \n",
        "    rho, pvalue = spearmanr(trgs, preds)\n",
        "    print(\"Evaluated on %d of %d examples\" % (n_exs, len(word_pairs)))\n",
        "    return rho\n",
        "def vector_length(u):\n",
        "    \"\"\"Length (L2) of the 1d np.array `u`. Returns a new np.array with the \n",
        "    same dimensions as `u`.\"\"\"\n",
        "    return np.sqrt(np.dot(u, u))\n",
        "  \n",
        "def cosine(u, v):        \n",
        "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
        "    the same dimensionality. Returns a float.\"\"\"\n",
        "    return 1.0 - (np.dot(u, v) / (vector_length(u) * vector_length(v)))\n",
        "  \n",
        "def prob_norm(u):\n",
        "    \"\"\"Normalize 1d np.array `u` into a probability distribution. Assumes \n",
        "    that all the members of `u` are positive. Returns a 1d np.array of \n",
        "    the same dimensionality as `u`.\"\"\"\n",
        "    return u / np.sum(u)\n",
        "\n",
        "def rowwise_norm_mat(mat):\n",
        "    return np.array([prob_norm(u) for u in mat])\n",
        "norm_mat = rowwise_norm_mat(mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kcujyo-sMtRt",
        "colab_type": "code",
        "outputId": "87b5c228-6861-44ea-c560-d92d944836d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "evaluation1=evaluate_word_similarity(test_data, test_trgs, norm_mat, tok2idx)\n",
        "evaluation1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluated on 248 of 771 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.015468056503250562"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "EBpRJMbPOE2X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we evaluate when context_size=2."
      ]
    },
    {
      "metadata": {
        "id": "ZfvLoH0QMDl6",
        "colab_type": "code",
        "outputId": "a8f310dd-268c-490b-bdfb-29a972f836f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "mat, tok2idx, idx2tok = build_cooccurrence_matrix(data, max_vocab_size=10000,context_size=2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting words...\n",
            "\tFinished counting words in 0.46556\n",
            "Pruning vocabulary...\n",
            "\tFinished pruning vocabulary in 0.00000\n",
            "Building co-occurrence matrix...\n",
            "\tFinished building co-occurrence matrix in 51.72593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S_DjmHrxPHy1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_word_similarity(word_pairs, targets, mat, tok2idx):\n",
        "    \"\"\" \"\"\"\n",
        "    preds = []\n",
        "    trgs = []\n",
        "    n_exs = 0\n",
        "    for (word1, word2), trg in zip(word_pairs, targets):\n",
        "        if word1 in tok2idx and word2 in tok2idx:\n",
        "            pred_sim = 1 - cosine(mat[tok2idx[word1]], mat[tok2idx[word2]])\n",
        "            preds.append(pred_sim)\n",
        "            trgs.append(trg)\n",
        "            n_exs += 1\n",
        "    \n",
        "    rho, pvalue = spearmanr(trgs, preds)\n",
        "    print(\"Evaluated on %d of %d examples\" % (n_exs, len(word_pairs)))\n",
        "    return rho\n",
        "def vector_length(u):\n",
        "    \"\"\"Length (L2) of the 1d np.array `u`. Returns a new np.array with the \n",
        "    same dimensions as `u`.\"\"\"\n",
        "    return np.sqrt(np.dot(u, u))\n",
        "  \n",
        "def cosine(u, v):        \n",
        "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
        "    the same dimensionality. Returns a float.\"\"\"\n",
        "    return 1.0 - (np.dot(u, v) / (vector_length(u) * vector_length(v)))\n",
        "  \n",
        "def prob_norm(u):\n",
        "    \"\"\"Normalize 1d np.array `u` into a probability distribution. Assumes \n",
        "    that all the members of `u` are positive. Returns a 1d np.array of \n",
        "    the same dimensionality as `u`.\"\"\"\n",
        "    return u / np.sum(u)\n",
        "\n",
        "def rowwise_norm_mat(mat):\n",
        "    return np.array([prob_norm(u) for u in mat])\n",
        "norm_mat = rowwise_norm_mat(mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oEoPP6OCMtLq",
        "colab_type": "code",
        "outputId": "539c0f9c-e153-47e2-8971-416a9279c84e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "evaluation2=evaluate_word_similarity(test_data, test_trgs, norm_mat, tok2idx)\n",
        "evaluation2"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluated on 248 of 771 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.015468056503250562"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "YJ1MweGCOGRM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we evaluate when context_size=3."
      ]
    },
    {
      "metadata": {
        "id": "66yiJuiaN93p",
        "colab_type": "code",
        "outputId": "a90a1c3d-bffc-420f-e66b-9d50435b5d9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "mat, tok2idx, idx2tok = build_cooccurrence_matrix(data, max_vocab_size=10000,context_size=3)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting words...\n",
            "\tFinished counting words in 0.53564\n",
            "Pruning vocabulary...\n",
            "\tFinished pruning vocabulary in 0.00000\n",
            "Building co-occurrence matrix...\n",
            "\tFinished building co-occurrence matrix in 51.25811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "agyvhblYPxN3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_word_similarity(word_pairs, targets, mat, tok2idx):\n",
        "    \"\"\" \"\"\"\n",
        "    preds = []\n",
        "    trgs = []\n",
        "    n_exs = 0\n",
        "    for (word1, word2), trg in zip(word_pairs, targets):\n",
        "        if word1 in tok2idx and word2 in tok2idx:\n",
        "            pred_sim = 1 - cosine(mat[tok2idx[word1]], mat[tok2idx[word2]])\n",
        "            preds.append(pred_sim)\n",
        "            trgs.append(trg)\n",
        "            n_exs += 1\n",
        "    \n",
        "    rho, pvalue = spearmanr(trgs, preds)\n",
        "    print(\"Evaluated on %d of %d examples\" % (n_exs, len(word_pairs)))\n",
        "    return rho\n",
        "def vector_length(u):\n",
        "    \"\"\"Length (L2) of the 1d np.array `u`. Returns a new np.array with the \n",
        "    same dimensions as `u`.\"\"\"\n",
        "    return np.sqrt(np.dot(u, u))\n",
        "  \n",
        "def cosine(u, v):        \n",
        "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
        "    the same dimensionality. Returns a float.\"\"\"\n",
        "    return 1.0 - (np.dot(u, v) / (vector_length(u) * vector_length(v)))\n",
        "  \n",
        "def prob_norm(u):\n",
        "    \"\"\"Normalize 1d np.array `u` into a probability distribution. Assumes \n",
        "    that all the members of `u` are positive. Returns a 1d np.array of \n",
        "    the same dimensionality as `u`.\"\"\"\n",
        "    return u / np.sum(u)\n",
        "\n",
        "def rowwise_norm_mat(mat):\n",
        "    return np.array([prob_norm(u) for u in mat])\n",
        "norm_mat = rowwise_norm_mat(mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RZGuPTJDN9_H",
        "colab_type": "code",
        "outputId": "e127b7b7-0d29-43df-f7c7-982910893c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "evaluation3=evaluate_word_similarity(test_data, test_trgs, norm_mat, tok2idx)\n",
        "evaluation3"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluated on 248 of 771 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.015468056503250562"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "xK9viygFOKFF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we evaluate when context_size=4."
      ]
    },
    {
      "metadata": {
        "id": "pQAncoEKN99S",
        "colab_type": "code",
        "outputId": "3271d111-657a-482b-8fed-94006901bbb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "mat, tok2idx, idx2tok = build_cooccurrence_matrix(data, max_vocab_size=10000,context_size=4)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting words...\n",
            "\tFinished counting words in 0.42914\n",
            "Pruning vocabulary...\n",
            "\tFinished pruning vocabulary in 0.00000\n",
            "Building co-occurrence matrix...\n",
            "\tFinished building co-occurrence matrix in 51.46432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vuLx3gpdP0ye",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_word_similarity(word_pairs, targets, mat, tok2idx):\n",
        "    \"\"\" \"\"\"\n",
        "    preds = []\n",
        "    trgs = []\n",
        "    n_exs = 0\n",
        "    for (word1, word2), trg in zip(word_pairs, targets):\n",
        "        if word1 in tok2idx and word2 in tok2idx:\n",
        "            pred_sim = 1 - cosine(mat[tok2idx[word1]], mat[tok2idx[word2]])\n",
        "            preds.append(pred_sim)\n",
        "            trgs.append(trg)\n",
        "            n_exs += 1\n",
        "    \n",
        "    rho, pvalue = spearmanr(trgs, preds)\n",
        "    print(\"Evaluated on %d of %d examples\" % (n_exs, len(word_pairs)))\n",
        "    return rho\n",
        "def vector_length(u):\n",
        "    \"\"\"Length (L2) of the 1d np.array `u`. Returns a new np.array with the \n",
        "    same dimensions as `u`.\"\"\"\n",
        "    return np.sqrt(np.dot(u, u))\n",
        "  \n",
        "def cosine(u, v):        \n",
        "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
        "    the same dimensionality. Returns a float.\"\"\"\n",
        "    return 1.0 - (np.dot(u, v) / (vector_length(u) * vector_length(v)))\n",
        "  \n",
        "def prob_norm(u):\n",
        "    \"\"\"Normalize 1d np.array `u` into a probability distribution. Assumes \n",
        "    that all the members of `u` are positive. Returns a 1d np.array of \n",
        "    the same dimensionality as `u`.\"\"\"\n",
        "    return u / np.sum(u)\n",
        "\n",
        "def rowwise_norm_mat(mat):\n",
        "    return np.array([prob_norm(u) for u in mat])\n",
        "norm_mat = rowwise_norm_mat(mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XDHUarhCN917",
        "colab_type": "code",
        "outputId": "2c22fecd-da7a-439d-997d-1e1554a8f649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "evaluation4=evaluate_word_similarity(test_data, test_trgs, norm_mat, tok2idx)\n",
        "evaluation4"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluated on 248 of 771 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.015468056503250562"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "C1K1A7jgRMc1",
        "colab_type": "code",
        "outputId": "bf3f7075-9c65-4cfe-98b8-f77c9be73485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "x=[1,2,3,4]\n",
        "y=[evaluation1,evaluation2,evaluation3,evaluation4]\n",
        "plt.scatter(x,y)\n",
        "plt.xlim(0, 5)\n",
        "plt.ylim(-0.01546, -0.01548)\n",
        "plt.title(\"scatter plot for evaluations when context_size=1,2,3,4\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'scatter plot for evaluations when context_size=1,2,3,4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEHCAYAAACa4PC5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3X28XVV95/HPLRCUGCTKVVJ0YLDw\nRUdnRnlIKaNgkUhF1GGgOIm1CYaKRWgGxKJUDNiWFnwYqhWTBs1IRTFoVCJjCCkPBghEZKBR+KrB\nJx4KoZqIgASSO3+sdTmHw7n33JzAvki+79frvl5n77X2Xmuve+7+nfVw9h0YGhoiIiLi6fY7412B\niIjYOiTgREREIxJwIiKiEQk4ERHRiASciIhoRAJOREQ0YtvxrkD0T9L2wDG2P1+3jwH+r+1fPU3l\nDQEvtX3nKHkEvNj2NV3S/hw4A/ik7b95Ouq4OSTtDvzIdt9/B+1tLunzwCLblz5VdeyzTgsp1/XX\n41mPsZA0FXjY9q3jdPxy4FTb3+3n+B7nfhmwCPiF7TeMkGdb4BPAoZQOwL8A77X9WEe+7YDzgD9s\ny3ei7UdHKf8SYGfbB2/51Tw10sP57fZq4J1t22cCO45TXYb9d+B1I6T9D+D0Z0KweQo93ua23zne\nwea30CzgP4/X8bYPeZqCjYAlwKoeWecAolzDK+vPrC753ge8CPhPNe9/AY4bpfzDgX03u+JPs/Rw\nGlY/0XwGeC2wDXArMLN+Qn4n8Fc16w3AbNuPSJoNnEL5fd0D/AnwG2AxsKOkbwM/pLxxr5I0E1gN\nfBKYWo/7iO3P1ToMAR8EZgKvsL2xrX4LgV8C/xXYC7gJeLvthzqu4yTgeMqHFgOzgd8HPgBskDTZ\n9ilt+c8BDgBeLumlwN8B/xt4PbAJuAx4v+2Nkn4CfBaYARxq+2dt5xkAPlTTngN8DTgZeDfwR7aP\nqPm2Ae4F/hswBFwAvBDYDviQ7S92XM9c4CW2Z3du15vHk46X9NmONv9rYIHtf5Z0MPBxYAdgPXCC\n7e/UfIcDv6K8Bx4Djrb9PUkHUT7tPgcYAM6wvaitjs8F1gKDth+W9JfASbZ3ren/APykZn+BpMso\nN7DbgKNsPyDpFcD5wBTgEWBWrdfBwNnAVcDbah1m2r6aDqO8T48GPkx5v90NHGd7TW3LnYFdKTfK\n+4G31p93Am+R9KJ67d1+ty8BbgT2sX2npOnAScDn24+3/fHOurbVebhu2wCP1na7qr7X3gHsA7yn\n7ZA9gbfZvlTSWym/24nAj4Dptu8fqazqN5TeyBuBl42S7xpgie0NtZ43UoJKp6spveeNwEZJ11Le\ne92udQfgXGAu5W/8GSM9nOa9EfiPwN6UN/X3gAPq8M5HgYMpb6SJwEn1D/FTlBvvnpQ3/Ids30u5\nuV9v+7W2j63nP9j2CuBjlBv53pSgc6akV7bVY8C22oNNm/8OHAW8FHg+HZ+kJP0+cGota2/gZ8DZ\n9dP9YuC89mADYPv9lJvG+23PpXyyeynlj+s1lJvv/2w75CW1fj/jid4B/DGwP+UP+WWUG8VXgdfX\nPzYovay7bd9e23WJ7ZcDxwIX1CGKsep6fJc2H26f51GGUk6s7XMOcJGk4b+3NwGftr0XcGVti+Fy\n/pftVwBvofweHmf7YeC7tD65vhb4eX3vDG8vr6+n1bbag/LJ+G21/K8Bn69lHw98vX4IgtJjXlmv\n89O0gsrjRnmf/gfgnyg36b2BbwLz2g49ul7ny4D7gGNtf4bWe+LjjPC7tf1TygeUcyRNBP6GEsw+\n3XH8aD4NHF6v7c8p7fs42+fZ3rvW/SzgZmCppD2AC4H/aXsPyu/rM7UtviTp9i4/O9j+qe17etQJ\n2zfW9+jwh9FDKUG8M991tn9U800B/ojSg+rmw7XOP+lVftPSw2neWuAVlJvJUtsfApD0Z8B1tu+u\n29OBx2w/JmnH4U9AwLcpPZxejgAOs70JWCvpq8CRlJ4PjPxmBfi67X+v9fga8AeU8eNhhwOX2L6v\nbi8ANnco6XDgo3Ws+jFJX6DcJP+5R/2OAD5re32t3wLKp9VPSfou5Q/265T2/XI95q2UHgPACsqn\n5ymbUdeRju8MhsOmAnfavhbA9lck/ROwe03/vu2b6uvvUm6yUG7E75R0b70JTe9y7ispH1CuBXYD\nvgQcKOkXtU7DcxmX2f4FgKTVlF7C3pTg89lar2slraX8fgEesP31tnrN7lL+NLq8T4E/Ba4cvilS\n3hPntAWza2rgQNLNwH/ocu6uv1vKB65/AFYCFwNfsv2vXY4fzX3A8ZI+Uz8crOiWqQaYcykfIjZI\nOgy4yvbw381ngHslbWP77ZtZhxHVnvungTtpvW+75bsG2I/ygfKKLumvonyo3Q848Kmq31MlPZyG\n2b4ROLH+/JukiyTtRBlyWNeW7zc12GwDnCXp+5JM+XQ3lt/bTsCXhz91UW7A7fM7vxjl2Pa0XwKT\nO9IH6/72PC8aQ5025xwj1W8n4H1t1/VR4Lk17RJan1zfSrk5QfkDvEbSD4DvU4LH5rz3N/f4zmuD\n8rsdvr71bfs3UoZ5oPSeHgKukPRDSUd1OfeVlKHJ/1zrcj3lxvIHwNW2hx+O2L5wZLiMnShDfLe1\ntd+LKEOFo9WrXdf3aec116AxUPOP9dwj/m5rT3w+5YPKP3U5tpe3ALsAN0m6uQ5fPkHt9X4R+IDt\nH7bV6XVtdbq+XssLO4/vVw3K/4fS4z9yhFEHAGy/Dngx8HJKr6/9PMNBa9TFBOMpPZxxYPsS4BJJ\nL6B82jwV+CmtT5pI2pHyx3YI5Y/ldbbvl3QcZYy7l7spwxure+Z8sp3bXr+AJ9/87+WJf3AvrPs2\nR7/nuBv4hu1PdUn7CvBBSftSVgb9sN5EFgF/bPsylZV9D3c5tvMmOBkevwmN5fgRr63eCF5Q9+89\n0kF1mPRE4ERJ04CvSvqW7V+3ZbseWEgZPruWMqR0LuV3tJzR3Q38qg4bPUGdwxmL++n+Pr2XEgiH\n90+mDOn2muvorF/X320dSns/pafz95QhujGzvQaYVYcV3wlcRJlTavfXwA9dV3221ekK208K/pK+\nRJnr7PSazjnPHv6J0oZvGSlQ1Hmkm23/zGW+dyHwEeAv27K9lDJHtqhMOzIBeJ6kW21vycKMp0x6\nOA2TNEvShwDqkMftlEntyyhDI7vXG9RngHdRPoH+pAabF1KGX55XT/coZdHA8HDPY5RPZFCGlY6v\nZW4r6ROSXjPGah4maafau3obZRiv3TeBI2t9oEzYf7OtTjvR2xLgXZK2qTeTP2k7x2i+DvzJ8FyN\npHdL+lMA23cBdwCn0xqWmFh/vlO3/wLYQKsNh90DvFLS70jamTLPMpbj29t82I3ALpKGb8BvpwyV\n/GSki5K0naSr6vg8lMUaj1Ju2o+z/Ui9xhnAtTUYbaL0wnoFnJ8Cdw73nCTtLOmLtf3HaqT36TJK\nT2CPmu944HJ3LO/tov39MuLvlrIa8KuURQR7Snpzl+O7kjQoaVkdmt5EGZob6sjzBsoowHs6Dl8K\nvHb4uiTtL+k8ANtvH5736fgZc7CRdCRliH16j17JW4G59f05QOnpPWEpeA1GO9rexfYulCH0654p\nwQYScMbD14F96pDJbZQ328ddvtvyZ5T19T+g/EF8nNLFf6GkH9XXfwW8VNLHKOPQvwvcXYPDl4Hr\nJP0xZbXP8+sw3PdorYgbi+WUP+47KcMkn21PrMOCfwd8uw4z7ES5yUOZyzle5TsAo/kk8PNat+9Q\nAtCiUY8ovlbL+G4t+y2Um8KwSyhB8su1rusok/Y317mDNfUcSyiBZNgi4MGafuFwXUY7vt6o29t8\nuH0epHww+FSt459TVvqN+L9A6s1mAbBc0vcpq5JOHOHmdSVl9dktdftGysq1NSOdv5YxRAl+7631\nugZYXus7JiO9T+v+2ZRFCLdTFm28ewynXAz8vaSPM8LvVtJ/oSxi+UgdbjoR+EeVxRntx49U57XA\nt4BVtW2/RAmS7T5A6dWuUmvy/+Q68X8csLj+vX6K1lDtiCQdX6/hbMqc2+0q39NC0nslfaRmfTdl\nbu9f28r9bJd876P0gm6jtPsulJGRznzPaAP5fzjRTr9FXxqM+G0k6XeBU9yxkvPpzvdMkB5ORESz\npjC2hQ9Pdb5x11cPp06kLqQsy9xI+fLYHR15ZlDW3W8C5tu+oO4/iDJccaztJXXfVZThjeGu/SnD\ny0breOUKYJntuZL+E6VbSy37ONs/7lbeWOoZT5QeTvw2Uvky6gdHSP4/ts9usj7RXb+r1KYD62zP\nqKtpzgaOGU6sY9tnUL7AtYEyLrqYMkZ6MmV1TadZI6yomk1ZbTHsTODvbC9V+Q7AX0o6ZYTyjhit\nnvFktmeOdx0iNlddWfb5nhljXPU7pHYIZbIOypePOr9gNBVYZXu9y7ejr6157qGsnFjPGNTVQtN5\n4jeW76e15HRy3R6pvF71jIiIhvTbw9mF8o15bG+SNCRpQtu34R9Pr+4DpgyvuKlrxDudVQPMbcCc\nGjjOoax+2qst3xmUHswZlJVX+1GWsD6pvDHU80mGhoaGBgYGRkqOiIjuet44ewYclQdHdj7iYupm\nFtQr/TzgVpcH/Z0PnKDyELuNtq+T1B5w/hb4oO0vSHovJQB9p+N8I5XXs0EGBgZYu/aBXtm2CoOD\nk9IWVdqiJW3RkrZoGRyc1DNPz4BjewHl+wGPqxPLuwC31In5gY5ew901fdiulC9bjVTG4rbNSynz\nLFOAfSWtpDw2Y3tJayjDYsPfrl1GWZ3xjRHKG67HSPWMiIiG9Dukdjnl0RJLKRPzV3ak3wAsUHlG\n2GOUIDGHLuoqtGWUx6evozyFdrXtc9vyzAR2t31h/YLdVMo3nvejPJZ/pPJ27FHPiIhoSL8B52Lg\nUEkrKP9TYyaApNMoDxC8vr5eSvkm8pm216v8U6BTKc+T2kfSSbanSZpP+Yb1g8BdlP/jMJJTgfMl\nvb+WfZzL/wbpVl7XekZERPPypIEnG8qYbJHx6Za0RUvaoiVt0TI4OKnnHHmeNBAREY1IwImIiEYk\n4ERERCMScCIiohEJOBER0YgEnIiIaEQCTkRENCIBJyIiGpGAExERjUjAiYiIRiTgREREIxJwIiKi\nEQk4ERHRiASciIhoRAJOREQ0IgEnIiIakYATERGNSMCJiIhGbNvPQZK2AxYCuwEbgVm27+jIMwOY\nA2wC5tu+oO4/CFgEHGt7Sd13FTAReLAefortm2raALACWGZ7rqRPAq+q+XYA1gEfAD7WVvwrgLcB\n04AZwF11/4XD9YiIiGb1FXCA6cA62zMkTQPOBo4ZTpQ0ETgD2B/YAKyStBiYDJwMXNvlnLNsr+6y\nfzYwYXjD9olt5XwY+H4NTgfXfTsBXwdWUgLOebY/1ed1RkTEU6TfIbVDgMX19RXAgR3pU4FVttfb\nfpgSYA4E7gGOBNaPpRBJO1OC27wuaZNrPS7pSHof8L9tbxrbpURERBP6DTi7AGsB6o19SNKEbunV\nfcAU2w/Z3jjCOc+SdI2keZKeW/edA5wOPNYl/3HA52wPDe+ox72R0sMZdrSkZZKWSPqPm3GNERHx\nFOo5pCZpNmVYq93Uju2BHqfplX4ecKvtNZLOB06QdCOw0fZ1kvbqcsx04ICOfW8DvtnWu7kM+Bfb\n10h6O/BJ4M096sLg4KReWbYaaYuWtEVL2qIlbTF2PQOO7QXAgvZ9khZSejG31AUEA7Y3tGW5u6YP\n25UypzJSGYvbNi+lzAdNAfaVtBIYBLaXtMb2hZL2BO6vw3Xt3gyc33beG9vSvgH8/WjXOmzt2gfG\nku1Zb3BwUtqiSlu0pC1a0hYtYwm8/S4auBw4GlgKHAFc2ZF+A7CgTuA/Rpm/mdPtRHUV2jLgKNvr\nKJP/q22f25ZnJrC77Qvrrv2AW7qcbj/g+LbjzgMusf3t4fNuzkVGRMRTp9+AczFwqKQVwCPATABJ\npwFX276+vl4KDAFn2l4v6XDgVGBvYB9JJ9meJmk+sFzSg5QlzHN7lD+FMi/UaSfb7R83FgDzJD1K\nWZ59XH+XGxERW2pgaGiod66ty1C6yEWGC1rSFi1pi5a0Rcvg4KRec/V50kBERDQjASciIhqRgBMR\nEY1IwImIiEYk4ERERCMScCIiohEJOBER0YgEnIiIaEQCTkRENCIBJyIiGpGAExERjUjAiYiIRiTg\nREREIxJwIiKiEQk4ERHRiASciIhoRAJOREQ0IgEnIiIakYATERGN2LafgyRtBywEdgM2ArNs39GR\nZwYwB9gEzLd9Qd1/ELAIONb2krrvKmAi8GA9/BTbN9W0AWAFsMz2XEmfBF5V8+0ArLM9TdKjwLVt\nVTiEElBHrWdERDSjr4ADTKfc6GdImgacDRwznChpInAGsD+wAVglaTEwGTiZJwaGYbNsr+6yfzYw\nYXjD9olt5XwY+H7dXG/74PYDJb1jtHpGRERz+h1SOwRYXF9fARzYkT4VWGV7ve2HKQHmQOAe4Ehg\n/VgKkbQzJbjN65I2udbjki2oZ0RENKTfHs4uwFoA25skDUmaYHtDZ3p1HzDF9kMAkrqd86waYG4D\n5tRAdQ5wOrBXl/zHAZ+zPVS3nyPpIsrw2Vdsf3wM9exqcHBSj8vfeqQtWtIWLWmLlrTF2PUMOJJm\nU4a12k3t2B7ocZpe6ecBt9peI+l84ARJNwIbbV8nqVvAmQ4c0Lb9PuCfgSHgGknX9FEPANaufWAs\n2Z71BgcnpS2qtEVL2qIlbdEylsDbM+DYXgAsaN8naSGl93BLXUAw0NFruLumD9sVWDlKGYvbNi+l\nzLNMAfaVtBIYBLaXtMb2hZL2BO6vvaDhc3ymrX7LKQsLhusxUj0jIqIh/Q6pXQ4cDSwFjgCu7Ei/\nAVggaSfgMcrcyZxuJ6qr0JYBR9leBxwMrLZ9bluemcDuti+su/YDbmlLF/BhYAawTS3vEuCRHvWM\niIiG9Lto4GJgG0krgBOADwBIOk3SAbXncRrlRn8FcKbt9ZIOr0ugDwPOlnR5nYOZDyyvw2AvBf6x\nR/lTKPNCANg28HPgRsoChcts3zhSPSMionkDQ0NDvXNtXYYyJltkfLolbdGStmhJW7QMDk7qOUee\nJw1EREQjEnAiIqIRCTgREdGIBJyIiGhEAk5ERDQiASciIhqRgBMREY1IwImIiEYk4ERERCMScCIi\nohEJOBER0YgEnIiIaEQCTkRENCIBJyIiGpGAExERjUjAiYiIRiTgREREIxJwIiKiEdv2c5Ck7YCF\nwG7ARmCW7Ts68swA5gCbgPm2L6j7DwIWAcfaXlL3XQVMBB6sh59i+6aaNgCsAJbZnivpk8Crar4d\ngHW2p0k6Bjillrfc9umSZgIfAdbU/Mts/00/1xwREVumr4ADTKfc6GdImgacDRwznChpInAGsD+w\nAVglaTEwGTgZuLbLOWfZXt1l/2xgwvCG7RPbyvkw8H1JOwB/TwlEvwZWSvpCzXax7ff1eZ0REfEU\n6XdI7RBgcX19BXBgR/pUYJXt9bYfpgSYA4F7gCOB9WMpRNLOlOA2r0va5FqPS2w/BLzK9gO2h4B/\nB1642VcVERFPm357OLsAawFsb5I0JGmC7Q2d6dV9wJQaGJDU7Zxn1QBzGzCnBqpzgNOBvbrkPw74\nXA0w2H6gnvtVwO7ASuBlwEGSvgVsB7zP9s29Lm5wcFKvLFuNtEVL2qIlbdGSthi7ngFH0mzKsFa7\nqR3bAz1O0yv9POBW22sknQ+cIOlGYKPt6yR1CzjTgQM66roncBEw3fajklYCa21/U9IBwOdpzf+M\naO3aB3pl2SoMDk5KW1Rpi5a0RUvaomUsgbdnwLG9AFjQvk/SQkov5pa6gGCgrXcDcHdNH7Yrpccx\nUhmL2zYvpcwHTQH2rUFjENhe0hrbF9bAcn/tBQ3X6SXA14A/sf3/6nlvB26vr6+XNChpG9sbe113\nREQ8tfodUrscOBpYChwBXNmRfgOwQNJOwGOU+Zs53U5UV6EtA46yvQ44GFht+9y2PDOB3W1fWHft\nB9zScaoLgPfY/m7bce8Hfm77i5JeSentJNhERIyDfgPOxcChklYAjwAzASSdBlxdexOnUQLSEHCm\n7fWSDgdOBfYG9pF0Ul3SPB9YLulB4C5gbo/yp1Dmhajl7gW8ljIPNLz745ThtQslHV+v9V19Xm9E\nRGyhgaGhofGuwzPNUMZki4xPt6QtWtIWLWmLlsHBSb3m6vOkgYiIaEYCTkRENCIBJyIiGpGAExER\njUjAiYiIRiTgREREIxJwIiKiEQk4ERHRiASciIhoRAJOREQ0IgEnIiIakYATERGNSMCJiIhGJOBE\nREQjEnAiIqIRCTgREdGIBJyIiGhEAk5ERDRi234OkrQdsBDYDdgIzLJ9R0eeGcAcYBMw3/YFdf9B\nwCLgWNtL6r6rgInAg/XwU2zfVNMGgBXAMttzJX0SeFXNtwOwzva0buWNpZ4REdGMvgIOMJ1yo58h\naRpwNnDMcKKkicAZwP7ABmCVpMXAZOBk4Nou55xle3WX/bOBCcMbtk9sK+fDwPdHKe+I0eoZERHN\n6XdI7RBgcX19BXBgR/pUYJXt9bYfpgSYA4F7gCOB9WMpRNLOlOA2r0va5FqPS0Ypr1c9IyKiIf32\ncHYB1gLY3iRpSNIE2xs606v7gCm2HwKQ1O2cZ9UAcxswpwaOc4DTgb265D8O+JztIUldyxtDPbsa\nHJw0WvJWJW3RkrZoSVu0pC3GrmfAkTSbMqzVbmrH9kCP0/RKPw+41fYaSecDJ0i6Edho+zpJ3QLO\ndOCAzSyvVz0AWLv2gbFke9YbHJyUtqjSFi1pi5a0RctYAm/PgGN7AbCgfZ+khZTewy11Yn6go9dw\nd00ftiuwcpQyFrdtXkqZZ5kC7CtpJTAIbC9pje0LJe0J3F97QaOVN7x/pHpGRERD+h1Suxw4GlhK\nmZi/siP9BmCBpJ2AxyhzJ3O6naiuQlsGHGV7HXAwsNr2uW15ZgK7276w7toPuGUM5e3Yo54REdGQ\nfhcNXAxsI2kFcALwAQBJp0k6oPY8TqPc6K8AzrS9XtLhdQn0YcDZki63PQTMB5ZLugZ4KfCPPcqf\nQpmnAWCk8kaqZ0RENG9gaGhovOvwTDOUMdki49MtaYuWtEVL2qJlcHBSzznyPGkgIiIakYATERGN\nSMCJiIhGJOBEREQjEnAiIqIRCTgREdGIBJyIiGhEAk5ERDQiASciIhqRgBMREY1IwImIiEYk4ERX\njzy6kXvuf5BHHt043lUZd2mLlrRFS9pi8/X77wniWWrjpk1c/C8/4uYfrOUXDzzCCyZtz6v3GuSY\nP/w9tvmdrevzSdqiJW3RkrboX54W/WRb9dOiL7riB1zxnTuftP8N+76E6W/o9o9Xn73SFi1pi5a0\nRXd5WnRslkce3cjNP1jbNe3mH9y/VQ0dpC1a0hYtaYstk4ATj1v/60f4xa8e6Zr2ywd+w/pfd097\nNkpbtKQtWtIWWyYBJx73/Odtzwt23L5r2uRJz+H5z+ue9myUtmhJW7SkLbZMAk48bvvttuHVew12\nTXv1Xjuz/XbbNFyj8ZO2aElbtKQttkxfq9QkbQcsBHYDNgKzbN/RkWcGMAfYBMy3fUHdfxCwCDjW\n9pK67ypgIvBgPfwU2zfVtAFgBbDM9ty67xPA64BHgBnABuALbcXvAZwGTAA+Aqyp+5fZ/pt+rnlr\nccwf/h5QxqN/+cBvmDzpObx6r50f3781SVu0pC1a0hb962uVmqQ/Bfa3fYKkacC7bB/Tlj4R+C6w\nPyUYrKIEiMnAxylB6IKOgPNe26u7lHUc8GfAN23PlfQm4N223yrpzcCutue15d8WuAo4DDgKeKXt\n923G5W3Vq9SGPfLoRraZsB0bNzy61X9qS1u0pC1a0hZP9HSuUjsEWFxfXwEc2JE+FVhle73th4Fr\na557gCOB9WMpRNLOwHRgXtvuI6i9GdtL2oNNNRP4iu1fj/lq4km2324bpuw8MX9IpC3apS1a0hab\nr98vfu4CrAWwvUnSkKQJtjd0plf3AVNsPwQgqds5z6oB5jZgTg1U5wCnA+2L23cH9pH0buBh4ATb\nP21Lnw1Ma9s+SNK3gO2A99m+udfFDQ5O6pVlq5G2aElbtKQtWtIWY9cz4EiaTbmJt5vasd2rK9Ur\n/TzgVttrJJ0PnCDpRmCj7esktQecAeCXtg+R9A7go8DRta4HALfb/lXNuxJYa/ubNe3zwKt61IUM\nqRWDg5PSFlXaoiVt0ZK2aBlL4O0ZcGwvABa075O0kNKLuaUuIBho690A3F3Th+1KufmPVMbits1L\ngWOAKcC+klYCg8D2ktYA9wJX17xLgQ+2HftmyhDf8HlvB26vr6+XNChpG9v5dlZERMP6HVK7nNKr\nWEqZU7myI/0GYIGknYDHKPM3c7qdqK5CWwYcZXsdcDCw2va5bXlmArvbvlDSo5QFAdcD+wBuO91+\nwJfajns/8HPbX5T0SkpvJ8EmImIc9BtwLgYOlbSCsjR5JoCk04Cra2/iNEpAGgLOtL1e0uHAqcDe\nlHmYk2xPkzQfWC7pQeAuYO4oZS8CPi3pWkowO64tbQplvmjYRcCFko6v1/quPq83IiK2UB7e+WRZ\nFl1lfLolbdGStmhJW7Tk4Z0REfGMkYATERGNSMCJiIhGJOBEREQjEnAiIqIRCTgREdGIBJyIiGhE\nAk5ERDQiASciIhqRgBMREY1IwImIiEYk4ERERCMScCIiohEJOBER0YgEnIiIaEQCTkRENCIBJyIi\nGpGAExERjdi2n4MkbQcsBHYDNgKzbN/RkWcGMAfYBMy3fUHdfxCwCDjW9pK67ypgIvBgPfwU2zfV\ntAFgBbDM9ty67xPA64BHgBm2fyzpJ8DPa32o+++qeX8fGAL+wvaqfq45IiK2TF8BB5gOrLM9Q9I0\n4GzgmOFESROBM4D9gQ3AKkmLgcnAycC1Xc45y/bqLvtnAxPazv0mYA/b+0h6MzANmFeT/8j2r9vy\nHgTsafsASS8HPgsc0Oc1R0TEFuh3SO0QYHF9fQVwYEf6VGCV7fW2H6YEmAOBe4AjgfVjKUTSzpTg\nNq9t9xHAFwBsL7E9r9uxbfWgACAsAAAK3klEQVT8Ws17GzBZ0o5jKTsiIp5a/fZwdgHWAtjeJGlI\n0gTbGzrTq/uAKbYfApDU7Zxn1QBzGzCnBqpzgNOBvdry7Q7sI+ndwMPACbZ/WtM+I2l3yhDcB2o9\nbmo7dm3d96vRLm5wcNJoyVuVtEVL2qIlbdGSthi7ngFH0mzKsFa7qR3bAz1O0yv9POBW22sknQ+c\nIOlGYKPt6yS1B5wB4Je2D5H0DuCjwNGUIbxvAb+g9Gr+Rx/1AGDt2gfGku1Zb3BwUtqiSlu0pC1a\n0hYtYwm8PQOO7QXAgvZ9khZSegq31AUEA229G4C7a/qwXYGVo5SxuG3zUsp80BRgX0krgUFge0lr\ngHuBq2vepcAH6zk+31a/y4BXdanH71KG9SIiomH9DqldTulVLKXMqVzZkX4DsEDSTsBjlPmbOd1O\nVFehLQOOsr0OOBhYbfvctjwzgd1tXyjpUeAw4HpgH8CSng98GTiiBr6DgEuAu4AzgXmSXgPcbTsf\nRyIixkG/Aedi4FBJKyhLk2cCSDoNuNr29fX1Uspy5DNtr5d0OHAqsDdlHuYk29MkzQeWS3qQEiTm\njlL2IuDTkq6lBLPj6rkvA1ZKehi4GbjE9pCkmyRdR1mefUKf1xsREVtoYGhoaLzr8EwzlDHZIuPT\nLWmLlrRFS9qiZXBwUs858jxpICIiGpGAExERjUjAiYiIRiTgREREIxJwIiKiEQk4ERHRiASciIho\nRAJOREQ0IgEnIiIakYATERGNSMCJiIhGJOBEREQjEnAiIqIRCTgREdGIBJyIiGhEAk5ERDQiASci\nIhqRgBMREY3Ytp+DJG0HLAR2AzYCs2zf0ZFnBjAH2ATMt31B3X8QsAg41vaSuu8qYCLwYD38FNs3\n1bQBYAWwzPbcuu8TwOuAR4AZtn8s6fXA2bU+BmbXPIuA79Xz/qvtE/u55oiI2DJ9BRxgOrDO9gxJ\n0yg3+mOGEyVNBM4A9gc2AKskLQYmAycD13Y55yzbq7vsnw1MaDv3m4A9bO8j6c3ANGAeMB94ve07\nJS0CDgMeAq62fVSf1xkREU+RfgPOIcDn6+srgM92pE8FVtleDyDpWuBAYDlwJHDBWAqRtDMluM0D\nXlJ3HwF8AWC4h1TtY/tX9fVa4IWUgBMREc8A/c7h7EK5qWN7EzAkaUK39Oo+YIrth2xvHOGcZ0m6\nRtI8Sc+t+84BTgcea8u3O7CPpOWSlkjardbjVwCSplB6PZfV/K+Q9A1JKyQd2uf1RkTEFurZw5E0\nmzKs1W5qx/ZAj9P0Sj8PuNX2GknnAydIuhHYaPs6SXt1nOuXtg+R9A7go8DRta4vAi4F/tz2v0v6\nIXAm8GVgD+BKSb9ne8NolRkcnNSjuluPtEVL2qIlbdGSthi7ngHH9gJgQfs+SQspvZhb6gKCgY6b\n+N01fdiuwMpRyljctnkpZT5oCrCvpJXAILC9pDXAvcDVNe9S4IO1TjsC/xc43fbl9bx3ARfXvGsk\n/Vuty49Hu+a1ax8YLXmrMTg4KW1RpS1a0hYtaYuWsQTefudwLqf0KpZS5lSu7Ei/AVggaSfKcNiB\nlBVrT1JXoS0DjrK9DjgYWG373LY8M4HdbV8o6VHKgoDrgX0oK9IAPgZ8wva32o6bQRnK+6ikXYAX\nA3f1ec0REbEF+g04FwOHSlpBWZo8E0DSaZRVYdfX10uBIeBM2+slHQ6cCuxNmYc5yfY0SfOB5ZIe\npASEuaOUvQj4dF2I8BhwnKQdgHcCe9YhQICLgC8CF0l6K2Wl23t6DadFRMTTY2BoaGi86/BMM5Qu\ncpHhgpa0RUvaoiVt0TI4OKnXXH2eNBAREc1IwImIiEYk4ERERCMScCIiohEJOBER0YgEnIiIaEQC\nTkRENCIBJyIiGpGAExERjUjAiYiIRiTgREREIxJwIiKiEQk4ERHRiASciIhoRAJOREQ0IgEnIiIa\nkYATERGNSMCJiIhGJOBEREQjtu3nIEnbAQuB3YCNwCzbd3TkmQHMATYB821fUPcfBCwCjrW9pO67\nCpgIPFgPP8X2TTVtAFgBLLM9t+77BPA64BFghu0fS3oD8Le1PpfZ/khb3t8HhoC/sL2qn2uOiIgt\n01fAAaYD62zPkDQNOBs4ZjhR0kTgDGB/YAOwStJiYDJwMnBtl3POsr26y/7ZwIS2c78J2MP2PpLe\nDEwD5gH/ALwRuAu4WtJXgEFgT9sHSHo58FnggD6vOSIitkC/Q2qHAIvr6yuAAzvSpwKrbK+3/TAl\nwBwI3AMcCawfSyGSdqYEt3ltu48AvgBge4nteZL2AH5h++e2NwGX1ToeAnyt5r0NmCxpx8292IiI\n2HL99nB2AdYC2N4kaUjSBNsbOtOr+4Apth8CkNTtnGfVAHMbMKcGqnOA04G92vLtDuwj6d3Aw8AJ\nI5T3MmBn4Ka2/Wtr3l+Ncm0Dg4OTRkneuqQtWtIWLWmLlrTF2PUMOJJmU4a12k3t2B7ocZpe6ecB\nt9peI+l84ARJNwIbbV8nqT3gDAC/tH2IpHcAHwU+McbyetUjIiKeJj0Dju0FwIL2fZIWUnoKt9QF\nBANtvRuAu2v6sF2BlaOUsbht81LKfNAUYF9JKylzMdtLWgPcC1xd8y4FPjhCeXdT5o/a9/8uZVgv\nIiIa1u+Q2uXA0ZQb/hHAlR3pNwALJO0EPEaZv5nT7UR1Fdoy4Cjb64CDgdW2z23LMxPY3faFkh4F\nDgOuB/YBbPsnknaUtDtwJ/BmYAZlSO1MYJ6k1wB3236gz2uOiIgt0G/AuRg4VNIKytLkmQCSTgOu\ntn19fb2Ushz5TNvrJR0OnArsTZmHOcn2NEnzgeWSHqSsMps7StmLgE9LupYSzI6r+98DfHG4frZ/\nAPxA0k2SrqMszz6hz+uNiIgtNDA0NDTedYiIiK1AnjQQERGNSMCJiIhG9DuH86yTR+A8kaRXAl8H\nPmH7U+Ndn/Ei6RzgtZS/lbNtf3WcqzQuJO1AeZzVi4HnAB8ZfjTV1krSc4HVlLZYOM7VGReSDqbM\nq3+v7vpX2yeOlD8Bh8ef75ZH4FT10USfBJaPd13Gk6TXA6+s74sXAjcDW2XAoaxG/Y7tcyTtRllZ\nulUHHOCvgF+MdyWeAa62fdRYMmZIrcgjcJ7oEeBNlO8ybc2uoSz/B1gHTJS0zTjWZ9zYvtj2OXXz\npZSvH2y1JO0NvAL45njX5bdJejjFLmz+I3CetWw/Bjw2wiOIthq2N9J6gvm7KE8h3ziOVRp39SsG\nL6F8121r9jHgvcCfjndFngFeIekbwAsoX4FZNlLG9HC6yyNw4nGS3koJOO8d77qMN9t/ALwF+Of6\npe2tjqR3Atfb/vF41+UZ4IeUL9e/lRJ8L5A0YaTM6eEUnY/GySNwAgBJb6Q8QPYw22N6yvmzkaR9\ngPvqE9n/n6RtKY+cum+cqzYeDgf2qP8e5SXAI5LutH3FONercbbvojwIAGCNpH+jPFqsazBOwCku\nJ4/AiQ6Sng+cC7zB9tY+Ofw6yj9cnCPpxcDzgPvHt0rjw3b7//6aC/xkaww28Pg/2pxi+6OSdqGs\nYrxrpPwJOEB9InUegVPVT7Mfo/wriEclHQUcuRXedI+hPI/vy23zWe+0/bPxq9K4+QxluOTbwHOB\nE+r/noqt2zeAi+qw8wTgPR0Pcn6CPNomIiIakUUDERHRiASciIhoRAJOREQ0IgEnIiIakYATERGN\nSMCJiIhGJOBEREQj/j9jZCv3JX/9NQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "iukiIboHG6Dj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__c__. Briefly discuss the pros and cons of varying (i) the context size (ii) the vocabulary size (iii) using bigrams instead of unigrams (iv) using subword tokens instead of words. [8 pts]"
      ]
    },
    {
      "metadata": {
        "id": "TCvlYg9nURo2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(i) Generally, narrower window sizes lead to\n",
        "better performance in syntactic tests while wider windows lead to\n",
        "better performance in semantic tests. (this is a note from cs224n)\n",
        "(ii)Performance increases with a larger vocabulary size; it takes more time to compute at the same time (iii)bigrams help machines learn meaning of words as two words combined might produce different meaning than seperated, however it only works well for word prediction if the test corpus looks like the training corpus (iv) using subword tokens reduces input size but loses some information"
      ]
    },
    {
      "metadata": {
        "id": "jSG95-FGG6Dl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Pointwise Mutual Information [20 pts]"
      ]
    },
    {
      "metadata": {
        "id": "OCk5XrkaG6Dn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In lecture, we introduced __pointwise mutual information__ (PMI), which addresses the issue of normalization removing information about absolute magnitudes of counts. The PMI for word $\\times$ context pair $(w,c)$ is \n",
        "\n",
        "$$\\log\\left(\\frac{P(w,c)}{P(w) \\cdot P(c)}\\right)$$\n",
        "\n",
        "with $\\log(0) = 0$. This is a measure of how far that cell's value deviates from what we would expect given the row and column sums for that cell."
      ]
    },
    {
      "metadata": {
        "id": "eGecCI7UG6Dn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__a__. Implement `pmi`, a function which takes in a co-occurence matrix and returns the matrix with PMI normalization applied. [15 pts]"
      ]
    },
    {
      "metadata": {
        "id": "n8OO61MXG6Dp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pmi(mat):\n",
        "    \"\"\"Pointwise mutual information\n",
        "    \n",
        "    args:\n",
        "        - mat: 2d np.array to apply PMI\n",
        "        \n",
        "    returns:\n",
        "        - pmi_mat: matrix of same shape with PMI applied\n",
        "       \n",
        "    \"\"\"\n",
        "    \n",
        "    sumofmat=np.sum(mat)\n",
        "    mat=mat/sumofmat\n",
        "    pmi = np.zeros((len(mat),len(mat[0])))\n",
        "    Pw= np.sum(mat,axis=1).tolist()\n",
        "    Pc= np.sum(mat,axis=0).tolist()\n",
        "    for i in range(0,len(mat)):\n",
        "      for j in range(0,len(mat[0])):\n",
        "        if mat[i][j]==0 or Pw[i]==0 or Pc[j]==0:\n",
        "          pmi[i][j]=0\n",
        "        else:\n",
        "          pmi[i][j]=np.log2(mat[i][j])-np.log2(Pw[i]*Pc[j])\n",
        "    return pmi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YuhjQEHMG6Dt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Apply PMI to the co-occurence matrix computed above with `context_size=1`. What is the PMI between the words \"the\" and \"end\"?"
      ]
    },
    {
      "metadata": {
        "id": "XuVjYXTT1fL3",
        "colab_type": "code",
        "outputId": "23794b4c-5242-4ae1-d851-862b98df0272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "mat_1"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[11162,     0,     0, ...,     0,     0,     1],\n",
              "       [    0,  9949,   466, ...,     0,     0,     0],\n",
              "       [    0,   466,  8395, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,     2,     0,     0],\n",
              "       [    0,     0,     0, ...,     0,     2,     0],\n",
              "       [    1,     0,     0, ...,     0,     0,     2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "vLc3dlagxhK_",
        "colab_type": "code",
        "outputId": "db093c85-e9fa-4187-dae3-743f10a0dc86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "cell_type": "code",
      "source": [
        "pmi_mat1=pmi(mat_1)\n",
        "pmi_mat1"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.89301893,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  2.23549729],\n",
              "       [ 0.        ,  2.91797937, -1.27537564, ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        , -1.27537564,  3.11854748, ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  0.        , ..., 15.02428358,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "        15.02428358,  0.        ],\n",
              "       [ 2.23549729,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        , 15.02428358]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "MS5Z_QtcA44I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "calculate the PMI between the words \"the\" and \"end\""
      ]
    },
    {
      "metadata": {
        "id": "VIjkyhOnA_WZ",
        "colab_type": "code",
        "outputId": "df949515-d802-4392-ad67-e2d2ef107e22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#find the index of 'end'\n",
        "list(tok2idx.keys()).index(\"end\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "V1q8am29BBMl",
        "colab_type": "code",
        "outputId": "2af4cb3e-39ca-4a81-9401-27e202d0bc1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#find the index of 'the'\n",
        "list(tok2idx.keys()).index(\"the\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "ehPV53Ve-krO",
        "colab_type": "code",
        "outputId": "2516aaf5-4396-4393-a7f5-264aef443721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pmi_mat1[2][201]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.491290363137491"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "vI_cnQCvG6Dt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__b__. We also consider an extension of PMI, positive PMI (PPMI), that maps all negative PMI values to 0.0 ([Levy and Goldberg 2014](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization)). \n",
        "Write `ppmi`, which is the same as `pmi` except it applies PPMI instead of PMI (feel free to implement it as an option of `pmi`). What is the PMI of the words \"the\" and \"start\"? The PPMI? [5 pts]"
      ]
    },
    {
      "metadata": {
        "id": "FLfL44QKHzS7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ppmi(mat):\n",
        "    \"\"\"Pointwise mutual information\n",
        "    \n",
        "    args:\n",
        "        - mat: 2d np.array to apply PMI\n",
        "        \n",
        "    returns:\n",
        "        - pmi_mat: matrix of same shape with PMI applied\n",
        "       \n",
        "    \"\"\"\n",
        "    \n",
        "    sumofmat=np.sum(mat)\n",
        "    mat=mat/sumofmat\n",
        "    ppmi = np.zeros((len(mat),len(mat[0])))\n",
        "    def sumcol(mat):\n",
        "      return [sum(col) for col in zip(*mat)]\n",
        "    def sumrow(mat):\n",
        "      return [sum(row) for row in zip(*mat)]\n",
        "    Pw= sumrow(mat)\n",
        "    Pc= sumcol(mat)\n",
        "    for i in range(0,len(mat)):\n",
        "      for j in range(0,len(mat[0])):\n",
        "        if mat[i][j]==0 or Pw[i]==0 or Pc[j]==0:\n",
        "          ppmi[i][j]=0\n",
        "        else:\n",
        "          if np.log2(mat[i][j])-np.log2(Pw[i]*Pc[j])>0:\n",
        "            ppmi[i][j]=np.log2(mat[i][j])-np.log2(Pw[i]*Pc[j])\n",
        "          else: ppmi[i][j]=0\n",
        "    return ppmi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RB5I1HTjJPIx",
        "colab_type": "code",
        "outputId": "8b137206-df90-4d26-ac44-36fb598f9097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "cell_type": "code",
      "source": [
        "ppmi_mat1=ppmi(mat_1)\n",
        "ppmi_mat1"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.89301893,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  2.23549729],\n",
              "       [ 0.        ,  2.91797937,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  3.11854748, ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  0.        , ..., 15.02428358,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "        15.02428358,  0.        ],\n",
              "       [ 2.23549729,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        , 15.02428358]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "Q-BMUSLvh3EB",
        "colab_type": "code",
        "outputId": "64a91c8d-0d64-4775-81d4-bf9c30391dbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#find the index of 'start'\n",
        "list(tok2idx.keys()).index(\"start\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "670"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "Lha4QstrCpsD",
        "colab_type": "code",
        "outputId": "bbddfdab-29ed-429e-b181-1f70a12c70c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ppmi_mat1[2][670]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4214900105719579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "_iekKAYSG6Du",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Analyzing PMI [25 pts]"
      ]
    },
    {
      "metadata": {
        "id": "6Xc2h9XCG6Dw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__a__. Consider the matrix `np.array([[1.0, 0.0, 0.0], [1000.0, 1000.0, 4000.0], [1000.0, 2000.0, 999.0]])`. Reweight this matrix using `ppmi`. (i) What is the value obtained for cell `[0,0]`, and (ii) give a brief description for what is likely problematic about this value. [10 pts]"
      ]
    },
    {
      "metadata": {
        "id": "qnmFwo52DM9W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "array1=np.array([[1.0, 0.0, 0.0], [1000.0, 1000.0, 4000.0], [1000.0, 2000.0, 999.0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RGk_jDnSDj0w",
        "colab_type": "code",
        "outputId": "4f5ea9f9-5161-4e44-818c-aa5710e6ba2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "ppmi(array1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        ],\n",
              "       [0.73624443, 0.15200309, 1.41532607],\n",
              "       [0.        , 0.41532607, 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "mdlpt4fGEZB1",
        "colab_type": "code",
        "outputId": "6c63fb5d-618b-4826-d154-5cfced340391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ppmi_array1=ppmi(array1)\n",
        "ppmi_array1[0][0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "SrMkP5zBLlzD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I'm not sure I noticed what's wrong but I assume some smoothing could be applied."
      ]
    },
    {
      "metadata": {
        "id": "PVdwr1BZG6Dx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__b__. Give a suggestion for dealing with the problematic value and explain why it deals with this. Demonstrate your suggestion empirically [10 pts]"
      ]
    },
    {
      "metadata": {
        "id": "gjNwNeNDLkdI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "array2=array1+100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lJSD48nSZ_vu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "51573767-efbd-4cfe-98ed-3cc7757077ee"
      },
      "cell_type": "code",
      "source": [
        "ppmi(array2)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.                 , 0.                 , 0.                 ],\n",
              "       [0.6590327456064049 , 0.13882770469731653, 1.3536939877679455 ],\n",
              "       [0.                 , 0.38845940592862194, 0.                 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "-N_fTeFVG6Dy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__c__. Consider starting with a word-word co-occurence matrix and applied PMI to this matrix. (i) Which of the following describe the resulting vectors: sparse, dense, high-dimensional, low-dimensional (ii) If you wanted the opposite style of representation, what could you do? [5 pts]\n"
      ]
    },
    {
      "metadata": {
        "id": "BfMgrVJuMS5w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(i)sparse, high-dimensional\n",
        "(ii)use truncated svd"
      ]
    },
    {
      "metadata": {
        "id": "MOMXfKrfG6D0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Word Analogy Evaluation [25 pts]"
      ]
    },
    {
      "metadata": {
        "id": "KEMOMlABG6D1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Word analogies provide another kind of evaluation for distributed representations. Here, we are given three vectors A, B, and C, in the relationship\n",
        "\n",
        "_A is to B as C is to __ _\n",
        "\n",
        "and asked to identify the fourth that completes the analogy. These analogies are by and large substantially easier than the classic brain-teaser analogies that used to appear on tests like the SAT, but it's still an interesting, demanding\n",
        "task. \n",
        "\n",
        "The core idea is that we make predictions by creating the vector\n",
        "\n",
        "$$(A - B) + C$$ \n",
        "\n",
        "and then ranking all vectors based on their distance from this new vector, choosing the closest as our prediction."
      ]
    },
    {
      "metadata": {
        "id": "mM3xXs7fG6D3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__a__. Implement the function `analogy_completion`. [9 pts]"
      ]
    },
    {
      "metadata": {
        "id": "N7VJ8-OMKlme",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I can't upload the glove file to google colab because it is too large. However the first part of hw1 can not run on my local jupyter notebook, so I wrote this part on my local jupyter notebook, so here is no output saved."
      ]
    },
    {
      "metadata": {
        "id": "2GjWwI2ZG6D6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def analogy_completion(a, b, c, vec):\n",
        "    \"\"\"Compute ? in \n",
        "    a is to b as c is to ? \n",
        "    as the closest to (b-a) + c\n",
        "    \"\"\"\n",
        "\n",
        "    vector_a=vec[a]\n",
        "    vector_b=vec[b]\n",
        "    vector_c=vec[c]\n",
        "\n",
        "    vector_d=vector_b-vector_a+vector_c\n",
        "    distance={}\n",
        "    for word in [x for x in vec.keys() if x not in [a,b,c]]:\n",
        "        distance[word]=cosine(vector_d,vec[word])\n",
        "\n",
        "    return min(distance.items(),key=operator.itemgetter(1))[0]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iho-wVT5G6D-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__b__. Our simple word embeddings likely won't perform well on this task. Let's instead look at some high quality pretrained word embeddings. Write code to load 300-dimensional [GloVe word embeddings](http://nlp.stanford.edu/data/glove.840B.300d.zip) trained on 840B tokens. Each line of the file is formatted as a word followed by 300 floats that make up its corresponding word embedding (all space delimited). The entries of GloVe word embeddings are not counts, but instead are learned via machine learning. Use your `analogy_completion` code to complete the following analogies using the GloVe word embeddings. [6 pts]"
      ]
    },
    {
      "metadata": {
        "id": "KrUy3k27Phtj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_glove(glove_file, n_vecs=20000):\n",
        "    \"\"\" \"\"\"\n",
        "    tok2vec = {}\n",
        "    with open(glove_file, 'r') as glove_fh:\n",
        "        for i, row in enumerate(glove_fh):\n",
        "            word, vec = row.split(' ', 1)\n",
        "            tok2vec[word] = np.array([float(n) for n in vec.split(' ')])\n",
        "            if i >= n_vecs:\n",
        "                break\n",
        "    return tok2vec\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cyuyxxn7Pieh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "glove_file = \"glove.840B.300d.txt\"\n",
        "glove_vecs = load_glove(glove_file, n_vecs=200000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fAl31SsePic6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "analogy_completion('study','hard','play',glove_vecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qPs9O57SPhqt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "analogy_completion('beijing','china','paris',glove_vecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aWvWpDA25uU0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "analogy_completion('gold','first','silver',glove_vecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pr0534En5um4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "analogy_completion('italian','mozzarella','american',glove_vecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x83KumjG5uST",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "analogy_completion('research','fun','engineering',glove_vecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Ei2M2UvG6D_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- \"Beijing\" is to \"China\" as \"Paris\" is to ?\n",
        "- \"gold\" is to \"first\" as \"silver\" is to ?\n",
        "- \"Italian\" is to \"mozzarella\" as \"American\" is to ?\n",
        "- \"research\" is to \"fun\" as \"engineering\" is to ?"
      ]
    },
    {
      "metadata": {
        "id": "JGG6AlEbG6EA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "c. Let's get a more quantitative, aggregate sense of the quality of GloVe embeddings. Load the analogies from `gram6-nationality-adjective.txt` and evaluate GloVe embeddings. Report the mean reciprocal rank of the correct answer (the last word on each line) for each analogy. [10 pts]"
      ]
    },
    {
      "metadata": {
        "id": "vK4BX-ytG6EB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Solution__"
      ]
    },
    {
      "metadata": {
        "id": "YGOC3cNPI7_w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_word_dataset(data_file):\n",
        "    with open(data_file, 'r') as data_fh:\n",
        "        raw_data = data_fh.readlines()\n",
        "    word1 = []\n",
        "    word2=[]\n",
        "    word3=[]\n",
        "    trgs = []\n",
        " \n",
        " \n",
        "    for i in range(0,len(raw_data),1): \n",
        " \n",
        "        list = []  \n",
        "        for word in raw_data:\n",
        "            \n",
        "            word=raw_data[i].split()\n",
        "            word1.append((word[0]))\n",
        "            word2.append((word[1]))\n",
        "            word3.append((word[2]))\n",
        "            trgs.append((word[3]))\n",
        "\n",
        "    return word1,word2,word3, trgs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ZzEsvZpG6EC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def analogy_evaluation(glove_vecs, test_file, verbose=False):\n",
        "    \"\"\"Basic analogies evaluation for a file `src_filename `\n",
        "    in `question-data/`.\n",
        "    \n",
        "    Parameters\n",
        "    ----------    \n",
        "    mat : 2d np.array\n",
        "        The VSM being evaluated.\n",
        "    rownames : list of str\n",
        "        The names of the rows in `mat`.\n",
        "        \n",
        "    src_filename : str\n",
        "        Basename of the file to be evaluated. It's assumed to be in\n",
        "        `vsmdata_home`/question-data.\n",
        "        \n",
        "    distfunc : function mapping vector pairs to floats (default: `cosine`)\n",
        "        The measure of distance between vectors. Can also be `euclidean`, \n",
        "        `matching`, `jaccard`, as well as any other distance measure \n",
        "        between 1d vectors.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    (float, float)\n",
        "        The first is the mean reciprocal rank of the predictions and \n",
        "        the second is the accuracy of the predictions.\n",
        "    \n",
        "    \"\"\"\n",
        "    def analogy2(a,b,c,d,vec):\n",
        "        \n",
        "\n",
        "        vector_a=vec[a]\n",
        "        vector_b=vec[b]\n",
        "        vector_c=vec[c]\n",
        "        vector_d=vector_b-vector_a+vector_c\n",
        "        distance={}\n",
        "        \n",
        "        for word in [x for x in vec.keys() if x not in [a,b,c]]:\n",
        "            distance[word]=cosine(vector_d,vec[word])\n",
        "            sorted_distance=sorted(distance.items(), key=lambda t: t[1])\n",
        "        for index,value in enumerate(sorted_distance):\n",
        "            if value[0]==d:\n",
        "                ind=index\n",
        "        return ind[0]\n",
        "\n",
        "    wordlist1, wordlist2,wordlist3, trglist=load_word_dataset(data_file)\n",
        "    n_exs = 0\n",
        "    for word1,word2,word3,trg in zip(wordlist1,wordlist2,wordlist3,trglist):\n",
        "        \n",
        "            rank_ = analogy2(word1,word2,word3,trglist,glove_vecs)+1\n",
        "            rank.append(rank_)\n",
        "            n_exs += 1\n",
        "    \n",
        "    rank=np.array(rank)\n",
        "    accuracy=sum(rank==1)/(len(rank))\n",
        "    mean=np.mean(1/rank)\n",
        "    \n",
        "\n",
        "    print(\"Evaluated on %d of %d examples\" % (n_exs, len(trglist)))\n",
        "    return accuracy,mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sODwx_smG6EG",
        "colab_type": "code",
        "outputId": "9a1f299e-0102-4c61-d06f-3af2f407c443",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "analogy_evaluation(glove_vecs, \"gram6-nationality-adjective.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9391509433962264, defaultdict(int, {True: 97, False: 9}))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}